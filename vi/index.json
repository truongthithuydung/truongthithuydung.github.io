[
{
	"uri": "/vi/4-transforming-glue/1-etl/",
	"title": "Xác thực dữ liệu và ETL (1)",
	"tags": [],
	"description": "",
	"content": "🛠 PART A: Tạo Glue Crawler cho Dữ liệu Tải Ban đầu 1. Điều hướng đến AWS Glue Console\nMở AWS Glue Console trong trình duyệt của bạn. 2. Chọn Crawlers\nTrong menu AWS Glue, dưới Data Catalog, chọn Crawlers. 3. Tạo Crawler: Nhấn vào nút Create crawler.\n4. Nhập Tên Crawler\nNhập glue-lab-crawler làm tên crawler cho tải dữ liệu ban đầu. Tùy chọn thêm một mô tả dễ nhận diện. Nhấn Next. 5. Chọn Nguồn Dữ liệu và Classifiers\nDưới Choose data sources and classifiers, chọn Not yet. Nhấn Add a data source. 6. Thêm Nguồn Dữ liệu\nTrên trang Add a data store, thực hiện các lựa chọn sau: Đối với Data source, chọn S3 từ danh sách thả xuống. Đối với Location of S3 data, chọn In this account. Đối với S3 path, duyệt đến thư mục đích chứa các tệp CSV. Ví dụ: s3://xxx-dmslabs3bucket-xxx/tickets/ Để mặc định cho các tham số còn lại. Nhấn Add an S3 data source. 7. Nhấn Next: Nhấn Next để tiếp tục.\n8. Cấu hình Cài đặt Bảo mật\nTrên trang Configure security settings, chọn: Dưới Existing IAM role, chọn \u0026lt;stackname\u0026gt;-GlueLabRole-\u0026lt;RandomString\u0026gt;, đã được tạo sẵn cho bạn.\nVí dụ: mod-6fccddd126456789-GlueLabRole-ZOQDII7JTBUM Nhấn Next. 9. Đặt Đầu Ra và Lịch Trình\nDưới Target database, nhấn Add database, sẽ mở một tab mới. Nhập ticketdata làm tên cơ sở dữ liệu và nhấn Create database. 10. Quay lại Trang Đặt Đầu Ra và Lịch Trình\nSau khi tạo cơ sở dữ liệu, quay lại trang Set output and scheduling: Chọn Target database là ticketdata.\n(Dùng refresh bên cạnh danh sách thả xuống để đảm bảo cơ sở dữ liệu mới được liệt kê.) Để trống trường Prefix added to tables (tùy chọn). Đối với Crawler schedule, chọn On demand làm tần suất. Nhấn Next. 11. Xem lại và Tạo Crawler\nXem lại thông tin và nhấn Create crawler. Crawler đã được tạo và sẵn sàng để chạy. 12. Chạy Crawler\nNhấn Run crawler để thực hiện crawler. Trạng thái sẽ chuyển từ Running sang Stopping. Chờ đến khi trạng thái trở lại Ready (quá trình này có thể mất vài phút). Sau khi hoàn thành, crawler sẽ tạo ra 15 bảng. 🚀 Chúc mừng! Bạn đã thành công trong việc tạo và thực thi Glue Crawler!\n🛠 PART B: Bài Tập Xác Thực Dữ Liệu 1. Mở Bảng Person\nTrong phần Tables của cơ sở dữ liệu ticketdata, nhấn vào bảng person. Bạn có thể nhận thấy rằng một số bảng, như bảng person, có tiêu đề cột mặc định như col0, col1, col2, col3. Những tên mặc định này được sử dụng khi không có tiêu đề hoặc khi crawler không thể xác định chúng. 2. Chỉnh Sửa Sơ Đồ (Schema)\nNhấn vào Edit Schema. 3. Sửa Tên Cột\nTrong phần Edit Schema, tìm dòng col0 (tên cột). Nhấn vào Edit bên cạnh col0. Gõ id làm tên cột mới và nhấn Save. Lặp lại quá trình này để đổi tên các cột còn lại thành các tên sau:\ncol1 → full_name col2 → last_name col3 → first_name 4. Lưu Thay Đổi\nSau khi tất cả tên cột đã được cập nhật, nhấn Save as new table version để áp dụng các thay đổi. "
},
{
	"uri": "/vi/2-clickstream-anomaly-detection-with-flink/1-prelab-setup/",
	"title": "Prelab Setup",
	"tags": [],
	"description": "",
	"content": "🌟 Giới thiệu Chào mừng bạn đến với pre-lab Phát hiện Anomalies Clickstream Thời gian Thực! Hướng dẫn này sẽ giúp bạn thiết lập môi trường để phát hiện anomalies trong thời gian thực sử dụng Amazon Managed Service for Apache Flink.\nKết thúc pre-lab, bạn sẽ có quyền truy cập vào các tài nguyên sau:\nHai Amazon S3 Buckets để lưu trữ dữ liệu thô và đã xử lý. Một Lambda function sẽ được kích hoạt khi phát hiện anomalies. Một SNS Topic gửi thông báo qua email/phone khi có anomalies xảy ra. Thông tin người dùng Amazon Cognito để đăng nhập vào Kinesis Data Generator để gửi dữ liệu. 🛠️ Triển khai CloudFormation Stack Bước 1: Chọn Khu Vực (Region) Sự kiện AWS: Giảng viên của bạn sẽ thông báo khu vực sử dụng. Tài khoản cá nhân: Chọn khu vực AWS của bạn (ví dụ: US-EAST-1 (N. Virginia)) và giữ nguyên khu vực này trong suốt workshop. Bước 2: Triển khai CloudFormation Template Nhấn nút Deploy to AWS dưới đây để thiết lập cơ sở hạ tầng cần thiết!\nDeploy to AWS\nBước 3: Điền Thông Tin Các Tham Số: Tên người dùng: Chọn một tên người dùng duy nhất (ví dụ: test). Mật khẩu: Tạo một mật khẩu với ít nhất 6 ký tự, bao gồm một chữ số và một chữ cái viết hoa. Email: Cung cấp email hợp lệ để nhận thông báo từ SNS. SMS: Nhập số điện thoại của bạn để nhận thông báo qua tin nhắn (ví dụ: +1XXXXXXXXX). Quan trọng: Đừng quên xác nhận rằng AWS CloudFormation sẽ tạo IAM resources bằng cách đánh dấu vào ô checkbox.\nBước 4: Nhấn Tạo! Sau vài phút, kiểm tra trạng thái CREATE_COMPLETE trong bảng điều khiển CloudFormation.\n📧 Xác Nhận Đăng Ký (Email \u0026amp; SMS) Kiểm tra Email của bạn:\nTìm kiếm email xác nhận đăng ký từ SNS. Nếu không thấy, kiểm tra thư mục spam! Nhấn vào liên kết xác nhận:\nXác nhận đăng ký và chuẩn bị nhận thông báo khi có anomalies được phát hiện. Khi stack của bạn được triển khai, nhấn vào tab Outputs để xem thêm thông tin: KinesisDataGeneratorUrl - URL của Kinesis Data Generator (KDG).\nRawBucketName - Tên bucket lưu trữ dữ liệu thô từ KDG.\nProcessedBucketName - Tên bucket lưu trữ dữ liệu đã biến đổi.\n🏗️ Thiết lập Amazon Kinesis Data Generator (KDG) Đăng nhập vào KDG\nVào tab Outputs trong CloudFormation và nhấn vào KinesisDataGeneratorUrl.\nĐăng nhập với tên người dùng và mật khẩu bạn đã tạo khi triển khai. Tạo Mẫu Payload cho Clickstream\nThiết lập các mẫu payload sau trong KDG:\nSchema Discovery Payload: {\u0026#34;browseraction\u0026#34;:\u0026#34;DiscoveryKinesisTest\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Click Payload: {\u0026#34;browseraction\u0026#34;:\u0026#34;Click\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Impression Payload: {\u0026#34;browseraction\u0026#34;:\u0026#34;Impression\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Lưu ý: Đừng gửi dữ liệu ngay bây giờ — chúng ta sẽ làm việc này trong lab chính. Các mẫu hiện đã sẵn sàng!\n📲 Xác Nhận Đăng Ký Email và SMS Vào SNS Topics\nTrong bảng điều khiển SNS, chọn Topics. Chọn ClickStreamEvent Topic\nBạn sẽ thấy email và số điện thoại của bạn đã được liệt kê dưới phần đăng ký. Nếu trạng thái là pending, hãy nhấp để xác nhận đăng ký. Xác Nhận:\nĐảm bảo bạn đã xác nhận qua liên kết trong email để bắt đầu nhận thông báo. 🧑‍💻 Quan sát Hàm Anomaly AWS Lambda Template CloudFormation đã triển khai một hàm Lambda sẽ được kích hoạt mỗi khi phát hiện anomaly.\nVào Console Lambda\nVào AWS Lambda và tìm CSEBeconAnomalyResponse dưới Functions. Xem Mã Lambda\nCuộn xuống phần mã nguồn. Bạn sẽ thấy giá trị TopicArn khớp với đăng ký email/SMS của bạn. Lambda sẽ gửi thông báo đến topic này khi phát hiện anomaly. 🎉 Chúc Mừng! Bạn Đã Hoàn Thành Prelab! Bây giờ bạn đã có tất cả mọi thứ cần thiết để tham gia vào lab chính! Bạn sẽ có thể phát hiện và xử lý anomalies thời gian thực trong dữ liệu clickstream sử dụng Amazon Managed Service for Apache Flink.\n"
},
{
	"uri": "/vi/1-introduction/1-local-system-setup/",
	"title": "Cài đặt hệ thống",
	"tags": [],
	"description": "",
	"content": "Thiết bị cần thiết cho phần thực hành trong workshop Để tham gia phần thực hành trong workshop này, bạn cần một laptop và một trình duyệt web.\n🌐 Yêu cầu về trình duyệt Hãy đảm bảo laptop của bạn đang chạy phiên bản mới nhất của một trong các trình duyệt web sau:\n✅ Mozilla Firefox\n✅ Google Chrome\n✅ Microsoft Edge\nViệc sử dụng trình duyệt cập nhật giúp đảm bảo khả năng tương thích với các dịch vụ AWS và mang đến trải nghiệm mượt mà trong suốt workshop.\n💡 Mẹo: Nếu gặp phải vấn đề, thử xóa bộ nhớ cache của trình duyệt hoặc chuyển sang sử dụng một trình duyệt khác.\n🚀 Khi hệ thống của bạn đã sẵn sàng, hãy tiếp tục với bước tiếp theo!\n"
},
{
	"uri": "/vi/1-introduction/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Chào mừng bạn đến với chuỗi phòng lab thực hành đầy thú vị về Khám phá quy trình ETL và Data Lake trên AWS! 🚀\nCác phòng lab này được thiết kế theo từng bước để hướng dẫn bạn thực hiện các quy trình quan trọng trong quản lý và phân tích dữ liệu bằng AWS.\n🌟 Bạn sẽ học được gì? Trong suốt workshop này, bạn sẽ thành thạo:\n✅ Nhập dữ liệu (Data Ingestion) – Tải và xử lý dữ liệu thô một cách hiệu quả.\n✅ Chuyển đổi dữ liệu (Data Transformations) – Làm sạch, làm giàu và cấu trúc dữ liệu để phân tích.\n✅ Khám phá Data Lake – Sử dụng SQL và công cụ trực quan hóa để khai thác dữ liệu.\n🏗️ Mô hình kiến trúc 📥 1. Thực hiện Nhập Dữ Liệu\n⚙️ 2. Thực hiện Chuyển đổi Dữ liệu\n🌊 3. Khám phá Data Lake\n🛠️ Yêu cầu thiết lập 💡 Trước khi bắt đầu:\nHãy đảm bảo bạn có quyền truy cập đầy đủ vào AWS Console. Làm theo hướng dẫn bên dưới để cấu hình Hệ thống cục bộ, Tài khoản AWS và Người dùng IAM. Nếu bạn tham gia một sự kiện AWS, sẽ có hướng dẫn bổ sung.\n🔹 Hướng dẫn thiết lập từng bước 1️⃣ Thiết lập hệ thống cục bộ – Chuẩn bị môi trường làm việc trên máy tính của bạn.\n2️⃣ Tạo tài khoản AWS và IAM User – Cấu hình thông tin xác thực và quyền truy cập AWS.\n3️⃣ Tham gia Workshop tại sự kiện AWS – Thực hiện các bước thiết lập cho sự kiện được AWS tổ chức.\n🚀 Bắt đầu ngay Workshop này tuân theo cách tiếp cận hướng dẫn từng bước, kèm theo gợi ý giúp bạn khắc phục sự cố. Nếu có bất kỳ câu hỏi nào, đội ngũ hướng dẫn viên của chúng tôi luôn sẵn sàng hỗ trợ!\n🔹 Làm theo hướng dẫn, thử nghiệm và học bằng cách thực hành.\n🔹 Đặt câu hỏi và khám phá các cách tiếp cận khác nhau.\n🔹 Hãy tận hưởng! Học Data Engineering là một hành trình thú vị!\n🔥 Hãy bắt đầu ngay thôi! 🚀\n"
},
{
	"uri": "/vi/4-transforming-glue/2-etl-2/",
	"title": "Xác thực dữ liệu và ETL (2)",
	"tags": [],
	"description": "",
	"content": "🛠 PART C: Bài Tập ETL Dữ Liệu Các Bước Tạo Công Việc ETL 1. Điều Hướng đến ETL Jobs\nTrong bảng điều hướng bên trái, nhấn vào ETL jobs. 2. Chọn Visual ETL\nNhấn vào Visual ETL. 3. Chọn Nguồn Dữ Liệu S3\nTrong danh sách Sources, chọn Amazon S3 để thêm Data source - S3 bucket node. 4. Hiển Thị Thuộc Tính Nguồn Dữ Liệu\nChọn Data source - S3 bucket node để xem thuộc tính nguồn dữ liệu. 5. Cấu Hình Thuộc Tính Nguồn Dữ Liệu\nTrong bảng bên phải, dưới Data source properties - S3, chọn Data Catalog table. Chọn cơ sở dữ liệu ticketdata từ menu thả xuống. Chọn bảng sport_team trong mục Table. 6. Thêm Biến Đổi Schema\nNhấn vào nút + và chọn Change Schema từ danh sách Transforms để thêm Transform - Change Schema node. 7. Sửa Đổi Kiểu Dữ Liệu Cột\nChọn Transform - Change Schema node để xem các thuộc tính. Trong bảng Transform panel bên phải, thay đổi data type của cột id thành double từ menu thả xuống. 8. Thêm S3 Data Target\nNhấn vào nút + một lần nữa và chọn Amazon S3 từ danh sách Targets để thêm Data target - S3 bucket node. Chọn Data target - S3 bucket node để xem các thuộc tính. Trong bảng bên phải: Thay đổi Format thành Parquet từ menu thả xuống. Trong Compression Type, chọn Uncompressed. Dưới S3 Target Location, chọn Browse S3. Duyệt đến bucket mod-xxx-dmslabs3bucket-xxx, chọn thư mục tickets và nhấn Choose. Trong hộp văn bản, thêm dms_parquet/sport_team/ vào URL S3. Đường dẫn sẽ giống như sau: s3://mod-xxx-dmslabs3bucket-xxx/tickets/dms_parquet/sport_team/ Đừng quên / ở cuối. Công việc sẽ tự động tạo thư mục. 9. Cấu Hình Chi Tiết Công Việc\nChọn tab Job details ở trên cùng. Nhập Glue-Lab-SportTeamParquet vào mục Name. Cho IAM Role, chọn vai trò có tên tương tự như mod-xxx-GlueLabRole-xxx. Cuộn xuống trang, và dưới mục Job bookmark, chọn Disable từ menu thả xuống. (Bạn có thể thử tính năng bookmark sau trong bài lab này.) 10. Lưu Công Việc\nNhấn Save ở góc trên bên phải để tạo công việc. 11. Chạy Công Việc\nSau khi công việc được tạo thành công, nhấn Run để bắt đầu công việc. 12. Giám Sát Công Việc\nChọn Jobs từ bảng điều hướng bên trái để xem danh sách công việc của bạn. Nhấn vào Monitoring để xem trạng thái các công việc đang chạy, tỷ lệ thành công/thất bại, và các số li 🛠 PART D: Tạo Glue Crawler Cho Các Tệp Parquet 1. Điều Hướng đến Crawlers trong Glue\nTrong menu điều hướng Glue, dưới Data Catalog, chọn Crawlers. Nhấn Create crawler. 2. Cung Cấp Tên Crawler\nNhập glue-lab-parquet-crawler vào mục Crawler name và nhấn Next. 3. Thêm Data Store\nTrong màn hình Add data store: Cho Choose a data store, chọn S3. Cho Location of S3 data, chọn In this account. Cho S3 path, nhập đường dẫn S3 (Thư mục Parquet chính) chứa các tệp Parquet lồng nhau, ví dụ: s3://xxx-dmslabs3bucket-xxx/tickets/dms_parquet/ Để tất cả các tham số khác ở mặc định. Nhấn Add an S3 data source. 4. Chọn Nguồn Dữ Liệu và Classifiers\nTrong màn hình Choose data sources and classifiers, nhấn Next. 5. Cấu Hình Các Thiết Lập Bảo Mật\nTrong trang Configure security settings, chọn Choose an existing IAM role. Cho IAM role, chọn vai trò đã có xxx-GlueLabRole-xxx. Nhấn Next. 6. Cấu Hình Đầu Ra và Lịch Trình\nTrong trang Set output and scheduling: Cho Target database, chọn cơ sở dữ liệu đã tạo trước đó, ví dụ: ticketdata. Cho Prefix added to tables (optional), nhập parquet_. Cho Crawler schedule, chọn On Demand. Nhấn Next. 7. Xem Lại và Tạo Crawler\nXem lại thông tin tổng quan và nhấn Create crawler. 8. Chạy Crawler\nSau khi tạo crawler, nhấn Run Crawler. Khi crawler hoàn thành, bạn sẽ thấy các bảng đã được thêm vào, tùy thuộc vào số lượng chuyển đổi Parquet ETL bạn đã thiết lập trong phần trước. 9. Kiểm Tra Các Bảng\nTrong bảng điều hướng bên trái, nhấn vào Tables. Thêm bộ lọc parquet cho Classification để hiển thị các bảng mới tạo. 🎉 Chúc Mừng! Bạn đã hoàn thành thành công bài lab Xác Minh Dữ Liệu và ETL.\n"
},
{
	"uri": "/vi/2-clickstream-anomaly-detection-with-flink/",
	"title": "Phát hiện bất thường trong luồng nhấp chuột thời gian thực với Amazon Managed Service for Apache Flink.",
	"tags": [],
	"description": "",
	"content": "🌟 Giới thiệu Dữ liệu streaming đang có mặt khắp mọi nơi! Nó liên tục được tạo ra từ hàng nghìn nguồn khác nhau, đến dưới dạng các mảnh nhỏ (kilobyte) và được xử lý ngay lập tức. Hãy nghĩ đến:\n📱 Nhật ký ứng dụng di động \u0026amp; web | 🛒 Giao dịch thương mại điện tử | 🎮 Hoạt động người chơi trong trò chơi\n📊 Dữ liệu giao dịch tài chính | 🌍 Theo dõi địa lý | 📡 Dữ liệu IoT và telemetry\nĐể hiểu được dữ liệu di chuyển nhanh này, các doanh nghiệp phân tích nó ngay khi dữ liệu được tạo ra, phát hiện các xu hướng, bất thường và thông tin chi tiết. Các công ty sử dụng phân tích streaming cho:\n✅ Thanh toán \u0026amp; Đo đếm – Theo dõi việc sử dụng ngay lập tức\n✅ Giám sát máy chủ \u0026amp; Ứng dụng – Phát hiện sự cố hệ thống trước khi chúng trở nên nghiêm trọng\n✅ Phân tích clickstream – Hiểu hành vi người dùng ngay lập tức\n✅ Cảm nhận trên mạng xã hội – Đáp ứng các xu hướng khi chúng xảy ra\n🏗️ Bộ công cụ AWS cho Streaming Amazon Web Services (AWS) cung cấp các công cụ mạnh mẽ để làm việc với dữ liệu streaming:\n🔹 Amazon Kinesis Data Streams – Thu thập dữ liệu theo thời gian thực\n🔹 Kinesis Data Firehose – Stream và tải dữ liệu vào kho lưu trữ hoặc phân tích\n🔹 Dịch vụ Quản lý Apache Flink – Xử lý thời gian thực \u0026amp; phát hiện bất thường\n🔹 Dịch vụ Streaming được Quản lý cho Kafka (MSK) – Stream và phân tích dữ liệu hiệu quả\n🛠️ Tổng quan về Lab Trong bài lab thực hành này, bạn sẽ làm việc với các dịch vụ AWS để xây dựng hệ thống phát hiện bất thường theo thời gian thực cho dữ liệu clickstream.\n🔹 Các nhiệm vụ trong Lab 1️⃣ Cài đặt trước lab – Cài đặt môi trường AWS của bạn.\n2️⃣ Phát hiện bất thường clickstream theo thời gian thực – Sử dụng Apache Flink để phát hiện bất thường theo thời gian thực.\n3️⃣ ETL streaming với AWS Glue – Biến đổi dữ liệu streaming hiệu quả.\n🎯 Bạn sẽ học được gì ✅ Cách thu thập, xử lý và phân tích dữ liệu streaming\n✅ Cách sử dụng Apache Flink, Kinesis, và MSK để xử lý theo thời gian thực\n✅ Cách phát hiện bất thường trong dữ liệu clickstream\n✅ Cách cài đặt và quản lý các dịch vụ AWS cho phân tích streaming\n💡 Sẵn sàng tham gia ngay? Hãy bắt đầu nào! 🚀\n"
},
{
	"uri": "/vi/2-clickstream-anomaly-detection-with-flink/2-real-time-clickstream-anomaly-detection/",
	"title": "Phát hiện Anomaly trong Clickstream Thời gian thực",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Hướng dẫn này giúp bạn hoàn thành Phát hiện Anomaly trong Clickstream Thời gian thực sử dụng Amazon Managed Service for Apache Flink.\nPhân tích lưu lượng web log để có cái nhìn sâu sắc giúp đưa ra quyết định kinh doanh đã được thực hiện theo phương pháp xử lý theo lô từ trước đến nay. Mặc dù phương pháp này hiệu quả, nhưng nó gây ra phản hồi chậm đối với các xu hướng mới nổi và hoạt động của người dùng.\nCó các giải pháp xử lý dữ liệu trong thời gian thực sử dụng streaming và micro-batching, nhưng chúng có thể phức tạp khi thiết lập và duy trì.\nAmazon Managed Service for Apache Flink là một dịch vụ quản lý giúp dễ dàng nhận diện và phản hồi những thay đổi trong hành vi dữ liệu theo thời gian thực.\nCác bước ✅ Thiết lập một Ứng dụng Amazon Analytics Studio thông qua triển khai CloudFormation stack ✅ Tạo lưu lượng truy cập website thời gian thực sử dụng Amazon Kinesis Data Generator (KDG) ✅ Thực hiện phân tích dữ liệu thời gian thực ✅ Dọn dẹp môi trường ✅ Phụ lục: Mã phát hiện Anomaly Trong thiết lập Kinesis prelab, bạn đã hoàn thành các yêu cầu tiên quyết cho bài lab này. Bây giờ, bạn sẽ tạo một pipeline Amazon Managed Service for Apache Flink.\n1️⃣ Thiết lập một Ứng dụng Amazon Analytics Studio thông qua triển khai CloudFormation stack Hãy chắc chắn rằng bạn đang ở khu vực AWS thích hợp. Nhấp vào liên kết Deploy to AWS ở đây để triển khai hạ tầng workshop trước bài lab. Nút trên sẽ mở một biểu mẫu “Quick create stack”. ✅ Chấp nhận các tham số mặc định ✅ Chọn ô kiểm để đồng ý với việc tạo mới vai trò IAM ✅ Nhấp Create Stack để chạy mẫu CloudFormation của Amazon Stack sẽ tạo sáu Kinesis Data Streams trong Amazon Kinesis Console: tickerstream – luồng thô để gửi lưu lượng ban đầu clickstream – ghi lại số lượng nhấp chuột impressionstream – ghi lại số lượng lượt hiển thị ctrstream – ghi lại tỷ lệ nhấp chuột (click-through rate) tính toán destinationstream – ghi lại điểm số anomaly anomalydetectionstream – ghi lại các bản ghi có điểm số anomaly lớn hơn 2 Mẫu sẽ tạo một ứng dụng Amazon Analytics Studio có tên là\nkda-flink-prelab-RealtimeApplicationNotebook trong Amazon Kinesis Application Console → tab Studio. Chạy Ứng dụng Studio: Chọn kda-flink-prelab-RealtimeApplicationNotebook dưới tab Studio. Chọn “Run” một lần nữa trên màn hình tiếp theo. 2️⃣ Tạo lưu lượng truy cập website thời gian thực sử dụng Amazon Kinesis Data Generator (KDG) Truy cập vào Amazon CloudFormation Console trong tài khoản AWS của bạn. Nhấp vào stack Kinesis-pre-lab đã được tạo trong Streaming Data Analytics Prelab setup. Đi đến tab Outputs để lấy liên kết Kinesis Data Generator.\nMở hai phiên làm việc đồng thời của Giao diện KDG trên trình duyệt của bạn.\nĐăng nhập bằng tên người dùng và mật khẩu đã nhập trong mẫu CloudFormation.\nPhiên 1: Gửi thông điệp impression với tốc độ 1 thông điệp mỗi giây trong 30 giây đến tickerstream. Nội dung thông điệp: {\u0026#34;browseraction\u0026#34;: \u0026#34;Impression\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;https://www.mysite.com\u0026#34;} Phiên 2: Gửi thông điệp click với tốc độ 5 thông điệp mỗi giây trong 30 giây đến tickerstream. Nội dung thông điệp: {\u0026#34;browseraction\u0026#34;: \u0026#34;Click\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;https://www.mysite.com\u0026#34;} Xem số lượng thông điệp đang được gửi đến luồng dữ liệu. Dừng gửi thông điệp sau 30 giây. 3️⃣ Thực hiện phân tích dữ liệu thời gian thực Truy cập vào Amazon Kinesis Application Console. Dưới tab Studio, chọn kda-prelab-template-RealtimeApplicationNotebook. Chọn \u0026ldquo;Open in Apache Zeppelin\u0026rdquo;. Trong Apache Zeppelin Console, chọn \u0026ldquo;Create new note\u0026rdquo;. Cung cấp tên notebook là kda_Interactive_notebook. Bây giờ, hãy thực hiện phân tích tương tác thời gian thực với các luồng dữ liệu Kinesis: ✔️ Tạo bảng Flink sử dụng câu lệnh Flink SQL\n✔️ Sử dụng câu lệnh Flink SQL để biến đổi và tạo các luồng dữ liệu mới trong thời gian thực\n✔️ Thực hiện phát hiện anomaly sử dụng Hàm Định nghĩa Người Dùng (UDF) của Flink và kích hoạt email thông báo anomaly trong thời gian thực\nCác bước Chạy các script tạo bảng. Chạy Hàm Định nghĩa Người Dùng (UDF) để Phát hiện Anomaly sử dụng Thuật toán Random Cut Forest. Xem dữ liệu truy cập website thời gian thực. Lọc impressionstream từ tickerstream. Lọc clickstream từ tickerstream. Tính toán Tỷ lệ Nhấp Chuột (CTR) và điền vào ctrstream. Xem Tỷ lệ Nhấp Chuột trong thời gian thực. Sử dụng UDF (Random Cut Forest) để tạo điểm số anomaly. Điền vào anomalydetectionstream. Kiểm tra điểm số anomaly từ thuật toán Random Cut Forest trong thời gian thực. Nhận thông báo email khi phát hiện anomaly. Khắc phục sự cố (Nếu không nhận được email anomaly) Mở hai phiên làm việc đồng thời của Giao diện KDG một lần nữa. Lặp lại các bước 2 - 10. Bạn sẽ bắt đầu nhận được thông báo email từ lần thử thứ hai. 4️⃣ Dọn dẹp Môi trường Sau khi hoàn thành lab, dừng ứng dụng của bạn: Nhấp vào Actions → Stop Application để tránh việc nhận quá nhiều tin nhắn SMS và email. Để xóa toàn bộ stack tài nguyên: Truy cập vào Amazon CloudFormation Console → Stacks Chọn kda-flink-pre-lab và nhấp vào Delete. Chọn Delete trên màn hình tiếp theo. Nếu thao tác Delete Stack thất bại do bucket S3 không trống: Chọn Delete lại. Trong pop-up, bạn có thể giữ lại các bucket S3 hoặc xóa chúng thủ công. Nhấp Delete lần nữa, và lần này, stack sẽ được xóa hoàn toàn. 📌 Phụ lục: Các Script Phát hiện Anomaly Mẫu Amazon CloudFormation:\n📌 1-Lab-Kinesis-Clickstream_CFN.yaml Apache Zeppelin Notebook:\n📌 kda_realtime_interactive_streaming_notebook.zpln Amazon Random Cut Forest Hàm Định nghĩa Người Dùng:\n📌 RandomCutForestUDF.java "
},
{
	"uri": "/vi/1-introduction/2-aws-account-setup/",
	"title": "Tài khoản AWS và người dùng IAM",
	"tags": [],
	"description": "",
	"content": "Để bắt đầu với các dịch vụ AWS, bạn cần có tài khoản AWS và một người dùng IAM (Identity and Access Management) với quyền quản trị viên. Dưới đây là tất cả những gì bạn cần biết:\n🛡️ AWS Identity and Access Management (IAM) AWS IAM cho phép bạn quản lý quyền truy cập vào các dịch vụ và tài nguyên AWS một cách an toàn. Với IAM, bạn có thể tạo và quản lý người dùng, nhóm và cấp quyền để cho phép hoặc từ chối quyền truy cập vào các tài nguyên AWS cụ thể.\nBYOA - Mang Tài Khoản Của Riêng Bạn Bạn thường cần mang theo tài khoản AWS của mình cùng với một người dùng IAM có quyền quản trị viên. Nếu có thể, tốt nhất nên sử dụng một tài khoản sạch hoặc \u0026ldquo;vanilla\u0026rdquo;.\nQuan trọng: Khi sử dụng tài khoản AWS của bạn (cá nhân hoặc tổ chức), hãy đảm bảo bạn hiểu các chính sách và tác động liên quan đến việc cấp phát tài nguyên.\nNếu bạn chưa có tài khoản AWS và muốn tham gia workshop này, bạn có thể tạo tài khoản miễn phí tại đây, hoặc yêu cầu Cloud Center of Excellence (CCoE) thiết lập một tài khoản cho bạn.\n🧑‍💻 Tạo Người Dùng IAM Quản Trị Viên Nếu bạn đã có tài khoản AWS nhưng chưa có người dùng IAM với quyền truy cập quản trị viên, hãy làm theo các bước dưới đây để tạo một người dùng:\nTạo tài khoản AWS nếu bạn chưa có tài khoản, bằng cách nhấp vào đây.\nKhi tài khoản của bạn đã sẵn sàng, hãy tạo một người dùng IAM mới cho workshop này bằng cách làm theo các bước sau:\nNhập thông tin người dùng và chọn tùy chọn \u0026ldquo;Programmatic access\u0026rdquo; cho loại quyền truy cập. Gắn Chính Sách IAM AdministratorAccess: Chính sách này cấp quyền truy cập quản trị đầy đủ vào tài nguyên AWS của bạn. Nếu cần, bạn có thể chọn quyền truy cập chi tiết cho các dịch vụ Amazon cụ thể được sử dụng trong workshop này. Nhấn Next → Bỏ qua Tags → Next Review. Nhấn Create User. Tải về tệp CSV chứa Access Key ID và Secret Key để sử dụng sau này. 💻 Cài Đặt AWS Command Line Interface (CLI) Trên Máy Của Bạn Để cài đặt AWS CLI, hãy làm theo hướng dẫn cài đặt chính thức trong tài liệu AWS CLI.\nSau khi cài đặt xong, liên kết AWS CLI với tài khoản AWS của bạn bằng thông tin IAM (Access Key và Secret Key) mà bạn đã tạo trước đó:\nMở terminal và chạy lệnh sau: aws configure Nhập các chi tiết sau khi được yêu cầu: AWS Access Key ID (từ tệp CSV). AWS Secret Access Key (từ tệp CSV). Default Region Name (ví dụ, us-east-1). Để trống Default Output Format, hoặc chọn định dạng như json. 🎉 Bạn Đã Sẵn Sàng! Bây giờ bạn đã cài đặt và cấu hình AWS CLI, bạn đã sẵn sàng tiếp tục với phần còn lại của workshop! 🚀\n📚 Tài Liệu Tham Khảo Thêm Tài liệu AWS IAM Tài liệu AWS CLI "
},
{
	"uri": "/vi/3-ingestion-with-dms/",
	"title": "Nhập dữ liệu",
	"tags": [],
	"description": "",
	"content": "Giới thiệu 📌 AWS DMS là gì? AWS Database Migration Service (AWS DMS) giúp bạn di chuyển cơ sở dữ liệu đến AWS một cách nhanh chóng và an toàn, với thời gian gián đoạn tối thiểu đối với các ứng dụng của bạn. Cơ sở dữ liệu nguồn vẫn hoàn toàn hoạt động trong suốt quá trình di chuyển, đảm bảo tính liên tục của doanh nghiệp.\n🔄 Các loại di chuyển được hỗ trợ AWS DMS hỗ trợ cả di chuyển cơ sở dữ liệu đồng nhất và không đồng nhất:\nDi chuyển đồng nhất (Cùng loại cơ sở dữ liệu)\nVí dụ: Oracle ➝ Oracle, MySQL ➝ MySQL Di chuyển không đồng nhất (Cơ sở dữ liệu khác nhau)\nVí dụ: Oracle ➝ Amazon Aurora, SQL Server ➝ Amazon RDS for PostgreSQL ⚡ Sao chép dữ liệu liên tục Với sao chép dữ liệu độ trễ thấp, AWS DMS cho phép bạn:\nStream dữ liệu từ nhiều nguồn vào Amazon S3 để xây dựng một data lake có độ sẵn sàng cao. Hợp nhất nhiều cơ sở dữ liệu vào một data warehouse quy mô petabyte bằng cách stream dữ liệu vào Amazon Redshift. 🏆 Lựa chọn 3: Bỏ qua Lab DMS Tổng quan các bước Mở AWS CloudShell Sao chép dữ liệu từ staging Amazon S3 bucket vào S3 bucket của bạn Kiểm tra dữ liệu trong Amazon S3 Về thiết lập lab Cơ sở dữ liệu nguồn là một phiên bản RDS PostgreSQL lưu trữ dữ liệu bán vé cho các sự kiện thể thao. AWS DMS (Database Migration Service) thường được sử dụng để di chuyển dữ liệu từ RDS vào một S3 bucket. Thay vì sử dụng DMS, bạn có thể sao chép trực tiếp dữ liệu nguồn vào S3 bucket của mình và tiếp tục với lab. Trong lab này, bạn sẽ: ✅ Sao chép dữ liệu từ S3 bucket trung tâm\n✅ Sử dụng AWS Glue Crawler để lập chỉ mục metadata\n✅ Biến đổi dữ liệu với AWS Glue\n✅ Truy vấn \u0026amp; tạo View trong Athena\n✅ Xây dựng bảng điều khiển với Amazon QuickSight Bước 1: Mở AWS CloudShell Khởi chạy AWS CloudShell trong tài khoản AWS của bạn. Nếu có pop-up xuất hiện, đóng lại. AWS CloudShell có tính năng theo khu vực, hãy kiểm tra Danh sách dịch vụ khu vực AWS để biết tính khả dụng. Bước 2: Sao chép dữ liệu từ Staging S3 Bucket Chạy lệnh sau trong CloudShell, thay \u0026lt;YourBucketName\u0026gt; bằng tên S3 bucket thực tế của bạn:\nexport dmslabs3bucket=\u0026lt;YourBucketName\u0026gt; aws s3 cp s3://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee/976050cc-0606-4b23-b49f-ca7b8ac4b153/v1/data/pg-source.zip /tmp \u0026amp;\u0026amp; unzip /tmp/pg-source.zip -d /tmp/ \u0026amp;\u0026amp; aws s3 cp --recursive --copy-props none /tmp/dms_sample s3://$dmslabs3bucket/tickets/dms_sample Kiểm tra dữ liệu trong S3 1. Truy cập S3 Console\nĐiều hướng đến AWS S3 Console và tìm dữ liệu đã sao chép qua CloudShell terminal. 2. Tìm S3 Bucket của bạn\nTên S3 bucket của bạn theo định dạng sau: BucketName/bucket_folder_name/schema_name/table_name/objects/ Ví dụ từ lab của chúng ta: /\u0026lt;BucketName\u0026gt;/tickets/dms_sample Mỗi table_name có đường dẫn riêng. 3. Xem một tệp bằng S3 Select\nĐiều hướng đến thư mục player. Chọn ô bên cạnh tên tệp. Nhấn vào menu Actions và chọn Query with S3 Select. Trên trang Query with S3 Select: Giữ nguyên các giá trị mặc định cho Input Settings và SQL Query. Nhấn Run SQL query. "
},
{
	"uri": "/vi/2-clickstream-anomaly-detection-with-flink/3-streaming-etl-with-aws-glue/",
	"title": "Xử lý ETL thời gian thực với AWS Glue",
	"tags": [],
	"description": "",
	"content": "🎓 Giới Thiệu Trong lab này, bạn sẽ học cách nhập, xử lý, và tiêu thụ dữ liệu streaming sử dụng các dịch vụ serverless của AWS như: Kinesis Data Streams, Glue, S3, Athena.\nĐể mô phỏng dữ liệu streaming đầu vào, chúng ta sẽ sử dụng Kinesis Data Generator (KDG).\n⚡ Thiết Lập Kinesis Stream Truy cập vào AWS Kinesis console. Nhấp vào Create data stream.\nĐiền thông tin:\nStream name: TicketTransactionStreamingData Capacity mode: Provisioned Provisioned shards: 2 Nhấp vào Create data stream. ✔️ 📝 Hướng Dẫn Lab 🔑 Tạo Bảng cho Nguồn Dữ Liệu Kinesis Stream trong Glue Data Catalog Truy cập vào AWS Glue console. Trong menu AWS Glue, chọn Data Catalog → Tables và nhấp vào nút Add table. Nhập TicketTransactionStreamData làm tên bảng. Nhấp vào Create database, đặt tên là tickettransactiondatabase, và nhấp vào Create. Từ dropdown, chọn tickettransactiondatabase và nhấp vào Next. Chọn Kinesis làm nguồn, sau đó: Chọn khu vực AWS phù hợp. Chọn TicketTransactionStreamingData từ dropdown stream. Đặt JSON làm định dạng dữ liệu đầu vào. Để schema trống (chúng ta sẽ bật phát hiện schema trong bước tiếp theo) và nhấp vào Next. Xem lại các cài đặt và nhấp vào Create. Nhấp vào bảng để xem các thuộc tính của nó. 💼 Tạo và Kích Hoạt Job Glue Streaming Trong Glue Console, nhấp vào Data Integration → ETL jobs. Nhấp vào Create job from a blank graph. Chọn Amazon Kinesis làm nguồn. Nhấp vào nút Amazon Kinesis trên canvas, sau đó cấu hình như sau: Name: Amazon Kinesis Source: Data Catalog table Database: tickettransactiondatabase Table: tickettransactionstreamdata Đảm bảo Detect schema được chọn. Để các trường khác ở mặc định. Nhấp vào nút + để thêm một node. Sau đó, chọn Amazon S3 trong tab Targets. Nhấp vào Data target - S3 bucket trên canvas, và cấu hình như sau: Format: Parquet Compression Type: Uncompressed S3 Target Location: Duyệt đến S3, chọn bucket có tên chứa dmslabs3bucket, và thêm TicketTransactionStreamingData/ vào đường dẫn S3. Đường dẫn sẽ trông như: s3://xxx-xxx-dmslabs3bucket-xxx/TicketTransactionStreamingData/ (đảm bảo có dấu / ở cuối). Dưới tab Job details, cấu hình như sau: Name: TicketTransactionStreamingJob IAM Role: Chọn xxx-GlueLabRole-xxx từ dropdown. Type: Spark Streaming Nhấn Save để tạo job. Sau khi job được tạo thành công, nhấp vào Run để bắt đầu xử lý dữ liệu. 🔄 💼 Tạo và Kích Hoạt Job Glue Streaming Trong Glue Console, nhấp vào Data Integration → ETL jobs. Nhấp vào Create job from a blank graph. Chọn Amazon Kinesis làm nguồn. Nhấp vào nút Amazon Kinesis trên canvas, sau đó cấu hình như sau: Name: Amazon Kinesis Source: Data Catalog table Database: tickettransactiondatabase Table: tickettransactionstreamdata Đảm bảo Detect schema được chọn. Để các trường khác ở mặc định. Nhấp vào nút + để thêm một node. Sau đó, chọn Amazon S3 trong tab Targets. Nhấp vào Data target - S3 bucket trên canvas, và cấu hình như sau: Format: Parquet Compression Type: Uncompressed S3 Target Location: Duyệt đến S3, chọn bucket có tên chứa dmslabs3bucket, và thêm TicketTransactionStreamingData/ vào đường dẫn S3. Đường dẫn sẽ trông như: s3://xxx-xxx-dmslabs3bucket-xxx/TicketTransactionStreamingData/ (đảm bảo có dấu / ở cuối). Dưới tab Job details, cấu hình như sau: Name: TicketTransactionStreamingJob IAM Role: Chọn xxx-GlueLabRole-xxx từ dropdown. Type: Spark Streaming Nhấn Save để tạo job. Sau khi job được tạo thành công, nhấp vào Run để bắt đầu xử lý dữ liệu. 🔄 🎥 Kích Hoạt Dữ Liệu Streaming từ KDG Khởi Động KDG\nMở KDG bằng URL bạn đã lưu từ khi thiết lập CloudFormation. Đăng nhập bằng tên người dùng và mật khẩu bạn đã cung cấp khi triển khai CloudFormation stack. Chọn Khu Vực và Kinesis Stream\nĐảm bảo bạn đã chọn khu vực phù hợp. Từ danh sách dropdown, chọn TicketTransactionStreamingData làm Kinesis stream mục tiêu. Để Records per second ở giá trị mặc định (100 records per second). Đối với record template, nhập NormalTransaction làm tên payload, và sao chép template payload như sau: { \u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;, \u0026#34;transactionAmount\u0026#34;: {{random.number( { \u0026#34;min\u0026#34;:10, \u0026#34;max\u0026#34;:150 } )}}, \u0026#34;sourceIp\u0026#34; : \u0026#34;{{internet.ip}}\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({ \u0026#34;weights\u0026#34; : [0.8,0.1,0.1], \u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;] } )}}\u0026#34;, \u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Gửi Dữ Liệu:\nNhấp vào Send data để kích hoạt dữ liệu giao dịch mô phỏng. 🚀 🛠️ Tạo Glue Crawler cho Dữ Liệu Đã Biến Đổi Điều hướng đến AWS Glue console. Trong menu AWS Glue, chọn Crawlers và nhấp vào Create crawler. Đặt tên cho crawler là TicketTransactionParquetDataCrawler và nhấp vào Next. Nhấp vào Add a datasource. Chọn S3 làm kho dữ liệu và nhấp vào Browse S3. Chọn bucket chứa dmslabs3bucket trong tên. Chọn thư mục TicketTransactionStreamingData và nhấp vào Choose. Sau khi cấu hình S3 bucket và đường dẫn dữ liệu, xem lại cài đặt và nhấp vào Add an S3 data source. Vì không cần nguồn dữ liệu khác, nhấp vào Next. Từ danh sách dropdown, chọn IAM role đã tồn tại có tên chứa GlueLabRole, sau đó nhấp vào Next. Chọn tickettransactiondatabase làm cơ sở dữ liệu đầu ra. Đặt parquet_ làm tiền tố cho bảng. Nhấp vào Next. Đặt crawler chạy mỗi giờ để đảm bảo các phân vùng mới được thêm vào. Xem lại cấu hình crawler và nhấp vào Create crawler. Sau khi crawler được tạo, chọn crawler và nhấp vào Run crawler để kích hoạt lần chạy đầu tiên. Chờ cho đến khi crawler hoàn tất công việc (trạng thái sẽ hiển thị là \u0026ldquo;Completed\u0026rdquo;). 🚨 Kích Hoạt Dữ Liệu Giao Dịch Bất Thường từ KDG Giữ cho dữ liệu KDG đang chạy: Mở một cửa sổ trình duyệt khác và khởi chạy KDG bằng URL bạn đã lưu từ khi thiết lập CloudFormation. Đăng nhập bằng tên người dùng và mật khẩu bạn đã cung cấp khi triển khai CloudFormation stack. Cấu hình dữ liệu streaming: Đảm bảo bạn đã chọn khu vực phù hợp. Từ danh sách dropdown, chọn TicketTransactionStreamingData làm Kinesis stream mục tiêu. Đặt Records per second là 1 (để mô phỏng một giao dịch mỗi giây). Chuẩn bị payload giao dịch bất thường:\nChọn Template 2 cho record template. Đặt payload name là AbnormalTransaction. Sao chép template payload sau vào trình soạn thảo: { \u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;, \u0026#34;transactionAmount\u0026#34;: {{random.number( { \u0026#34;min\u0026#34;:10, \u0026#34;max\u0026#34;:150 } )}}, \u0026#34;sourceIp\u0026#34; : \u0026#34;221.233.116.256\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({ \u0026#34;weights\u0026#34; : [0.8,0.1,0.1], \u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;] } )}}\u0026#34;, \u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Click \u0026ldquo;Send data\u0026rdquo; to trigger the abnormal transactions (1 transaction per second, all from the same source IP address). 🔍 Phát Hiện Giao Dịch Bất Thường bằng Truy Vấn Ad-Hoc từ Athena Điều hướng đến AWS Athena console:\nNếu đây là lần đầu tiên bạn sử dụng Athena, mở rộng phần Get Started using Athena bên dưới. Chọn nguồn dữ liệu và cơ sở dữ liệu:\nChọn AwsDataCatalog làm nguồn dữ liệu. Chọn tickettransactiondatabase làm cơ sở dữ liệu. Làm mới để đảm bảo bảng parquet_tickettransactionstreamingdata xuất hiện trong danh sách. Truy vấn số lượng giao dịch theo địa chỉ IP nguồn: Sao chép truy vấn sau để đếm số giao dịch từ mỗi source IP trong vòng một giờ qua: SELECT count(*) as numberOfTransactions, sourceip FROM \u0026#34;tickettransactiondatabase\u0026#34;.\u0026#34;parquet_tickettransactionstreamingdata\u0026#34; WHERE ingest_year=\u0026#39;2024\u0026#39; AND cast(ingest_year as bigint)=year(now()) AND cast(ingest_month as bigint)=month(now()) AND cast(ingest_day as bigint)=day_of_month(now()) AND cast(ingest_hour as bigint)=hour(now()) GROUP BY sourceip ORDER BY numberOfTransactions DESC; Thực thi truy vấn. Bạn sẽ thấy một số lượng lớn giao dịch đến từ cùng một địa chỉ IP nguồn. "
},
{
	"uri": "/vi/4-transforming-glue/",
	"title": "Chuyển đổi dữ liệu bằng Glue",
	"tags": [],
	"description": "",
	"content": "🎯 Giới thiệu Trong lab này, bạn sẽ khám phá AWS Glue, một dịch vụ tích hợp dữ liệu không máy chủ mạnh mẽ được thiết kế để đơn giản hóa quá trình khám phá, chuẩn bị, di chuyển và tích hợp dữ liệu từ nhiều nguồn khác nhau cho phân tích, học máy (ML) và phát triển ứng dụng.\nAWS Glue Crawlers: Các crawler này giúp điền đầy AWS Glue Data Catalog với các bảng, đây là phương pháp chính được hầu hết người dùng sử dụng. Crawler có thể quét nhiều kho dữ liệu trong một lần chạy. Data Catalog: Sau khi hoàn thành, crawler sẽ tạo hoặc cập nhật các bảng trong Data Catalog của bạn. ETL Jobs: Các công việc Extract, Transform và Load (ETL) mà bạn định nghĩa trong AWS Glue sử dụng các bảng trong Data Catalog này làm nguồn và đích. Các công việc này sẽ đọc từ và ghi vào các kho dữ liệu được chỉ định trong các bảng nguồn và đích. 🛠 Yêu cầu tiên quyết Hoàn thành Ingestion with DMS Lab (bất kỳ tùy chọn nào) là yêu cầu trước khi bắt đầu lab này. 📋 Tóm tắt các nhiệm vụ Trong lab này, bạn sẽ hoàn thành các nhiệm vụ sau. Bạn có thể chọn hoàn thành chỉ Data Validation và ETL để tiến tới lab tiếp theo, nơi bạn sẽ truy vấn các bảng bằng Amazon Athena và trực quan hóa chúng bằng Amazon QuickSight.\nNhiệm vụ: Data Validation và ETL (1) Data Validation và ETL (2) "
},
{
	"uri": "/vi/5-query-and-visualize/",
	"title": "Truy vấn và trực quan hóa",
	"tags": [],
	"description": "",
	"content": "🔍 Truy vấn Dữ liệu với Amazon Athena Bước 1: Mở Amazon Athena Trong AWS Services Console, tìm kiếm Athena. Nếu đây là lần đầu tiên bạn sử dụng Athena trong tài khoản AWS của mình, hãy mở rộng \u0026ldquo;Get Started using Athena\u0026rdquo; và làm theo hướng dẫn cài đặt. Bước 2: Chọn Cơ sở dữ liệu và Kiểm tra Bảng Trong Query Editor, chọn cơ sở dữ liệu mới tạo của bạn (ví dụ: \u0026quot;ticketdata\u0026quot;). Nhấp vào bảng \u0026ldquo;parquet_sporting_event_ticket\u0026rdquo; để kiểm tra các trường dữ liệu. Các kiểu dữ liệu của id, sporting_event_id, và ticketholder_id nên là (double). Bước 3: Truy vấn trên Nhiều Bảng Tiếp theo, chúng ta sẽ truy vấn trên các bảng sau:\nparquet_sporting_event parquet_sport_team parquet_sport_location 📌 Sao chép và chạy truy vấn SQL sau trong Athena Query Editor:\nSELECT e.id AS event_id, e.sport_type_name AS sport, e.start_date_time AS event_date_time, h.name AS home_team, a.name AS away_team, l.name AS location, l.city FROM parquet_sporting_event e, parquet_sport_team h, parquet_sport_team a, parquet_sport_location l WHERE e.home_team_id = h.id AND e.away_team_id = a.id AND e.location_id = l.id; 🔍 Bước 4: Tạo View Nhấp vào Create, sau đó chọn View from query. Đặt tên cho view là sporting_event_info và nhấp vào Create. View mới của bạn đã được tạo thành công. 🔍 Bước 5: Truy vấn View Mở một tab truy vấn mới trong Athena. Sao chép và dán truy vấn SQL sau vào Query Editor: SELECT t.id AS ticket_id, e.event_id, e.sport, e.event_date_time, e.home_team, e.away_team, e.location, e.city, t.seat_level, t.seat_section, t.seat_row, t.seat, t.ticket_price, p.full_name AS ticketholder FROM sporting_event_info e, parquet_sporting_event_ticket t, parquet_person p WHERE t.sporting_event_id = e.event_id AND t.ticketholder_id = p.id; 🔍 Bước 7: Lưu Truy vấn Nhấp vào Save as. Đặt tên truy vấn là create_view_sporting_event_ticket_info và nhập mô tả. Nhấp vào Save query. Quay lại Query Editor, bạn sẽ thấy tên truy vấn đã thay đổi. Nhấp vào Run để chạy truy vấn. Kết quả sẽ hiển thị bên dưới cửa sổ truy vấn. 🔍 Bước 8: Tạo Một View Khác Nhấp vào View from query. Đặt tên view là sporting_event_ticket_info. Nhấp vào Create. 🔍 Bước 9: Sao Chép và Chạy Truy Vấn Sao chép đoạn mã SQL sau vào tab Query 5: SELECT sport, count(distinct location) as locations, count(distinct event_id) as events, count(*) as tickets, avg(ticket_price) as avg_ticket_price FROM sporting_event_ticket_info GROUP BY 1 ORDER BY 1; Nhấn vào Save as, đặt tên cho truy vấn là analytics_sporting_event_ticket_info và thêm mô tả, sau đó nhấn Save. Kết quả của truy vấn Tạo Biểu Đồ QuickSight Phần này hướng dẫn bạn tạo các loại biểu đồ khác nhau trong QuickSight.\nBước 1: Điền Dữ Liệu Vào Biểu Đồ Trong Fields list, nhấn vào cột ticket_price để đưa dữ liệu vào biểu đồ. Nhấn vào biểu tượng mở rộng ở góc của trường ticket_price. Chọn Show as Currency để hiển thị số dưới dạng giá trị đô la. Bước 2: Thêm Biểu Đồ Nhấn vào nút Add trong Visuals pane. Trong Visual types, chọn biểu tượng Vertical bar chart. Thiết lập X-axis: Trong Fields list, chọn event_date_time. Biểu đồ sẽ tự động cập nhật. Thiết lập Y-axis (Value): Chọn ticket_price trong Fields list. Bước 3: Điều Chỉnh và Nhóm Dữ Liệu Kéo thả biểu đồ để điều chỉnh không gian trong dashboard. Trong Fields list, kéo và thả seat_level vào Group/Color. Sử dụng thanh trượt dưới trục X để hiển thị toàn bộ dữ liệu. Bước 4: Thay Đổi Loại Biểu Đồ Trong Visuals pane, nhấn vào Vertical bar chart. Trong Change visual type, chọn biểu tượng Clustered bar combo chart. Trong Fields list, kéo và thả ticketholder vào Lines. Trong Lines: Nhấn vào dropdown box. Chọn Aggregate: Count Distinct. Trục Y-axis sẽ cập nhật ở bên phải. Bước 5: Khám Phá Insight Tạo Tham Số (Parameters) Trong QuickSight Phần này hướng dẫn bạn tạo các tham số với bộ điều khiển cho bảng điều khiển và liên kết chúng vào bộ lọc trên tất cả biểu đồ.\nBước 1: Mở Mục Parameters Từ Toolbar, chọn biểu tượng Parameters. Bước 2: Tạo Tham Số EventFrom Nhấn Add để tạo tham số mới. Trong Name, nhập EventFrom. Trong Data type, chọn Datetime. Trong Time granularity, chọn Hour. Trong Default value, chọn ngày từ lịch tương ứng với ngày bắt đầu sớm nhất trong event_date_time của biểu đồ. Ví dụ: 2021-01-01 00:00. Nhấn Create, sau đó đóng hộp thoại Parameter Added. Bước 3: Tạo Tham Số EventTo Nhấn Add để tạo tham số khác. Thiết lập các thuộc tính sau: Name: EventTo Data type: Datetime Time granularity: Hour Default value: Chọn ngày từ lịch tương ứng với ngày kết thúc muộn nhất trong event_date_time của biểu đồ. Ví dụ: 2022-01-01 00:00. Nhấn Create. Bước 4: Thêm Điều Khiển (Controls) Trong cửa sổ tiếp theo, bạn có thể chọn bất kỳ tùy chọn nào để thực hiện các thao tác với tham số. Ngoài ra, nhấn vào ba chấm cho tham số EventFrom và chọn Add control. Trong Display name, nhập Event From và nhấn Add. Lặp lại quá trình để thêm điều khiển cho EventTo, đặt Display name là Event To. Bây giờ, bạn sẽ thấy và có thể mở rộng phần Controls phía trên biểu đồ.\nTạo Bộ Lọc QuickSight Để hoàn tất quá trình, chúng ta sẽ kết nối một bộ lọc với các điều khiển cho tất cả các biểu đồ.\nTừ Toolbar, chọn Filter. Nhấn vào biểu tượng cộng (+) để thêm bộ lọc cho trường event_date_time. Nhấn vào bộ lọc vừa thêm để chỉnh sửa các thuộc tính của nó. Đặt Applied To trong menu thả xuống thành All applicable visuals. Trong Filter type, chọn Date \u0026amp; Time range. Đặt Condition là Between. Chọn Use Parameter, nhấn Yes để áp dụng cho tất cả các biểu đồ. Đặt Start date parameter thành EventFrom. Đặt End date parameter thành EventTo. Nhấn Apply. Thêm Trường Tính Toán (Calculated Fields) Bây giờ, chúng ta sẽ thêm các trường tính toán cho ngày trong tuần và giờ trong ngày, sau đó trực quan hóa chúng bằng scatter plot.\nNhấn vào menu Data và chọn Add Calculated Field. Đặt tên trường là: event_day_of_week. Nhập công thức: extract('WD', {event_date_time}). Nhấn Save. Thêm một trường tính toán khác với các thuộc tính sau: Tên trường tính toán: event_hour_of_day Công thức: extract('HH', {event_date_time}) Nhấn vào biểu tượng Visualize từ Tool bar và chọn Add visual. Chọn loại trường là scatter plot.\nTrong Fields list, nhấn vào các thuộc tính sau để thiết lập các thuộc tính của biểu đồ:\nX-axis: \u0026quot;event_hour_of_day\u0026quot; Y-axis: \u0026quot;event_day_of_week\u0026quot; Size: \u0026quot;ticket_price\u0026quot; Bây giờ bạn có thể nhấn vào Publish ở góc trên bên phải của màn hình để xuất bản các biểu đồ. Bạn cũng có thể chia sẻ nó bằng cách nhấn vào menu Share.\n"
},
{
	"uri": "/vi/",
	"title": "Khám phá quy trình ETL và Data Lake trên AWS",
	"tags": [],
	"description": "",
	"content": "Khám phá quy trình ETL và Data Lake trên AWS 🔍 Khám phá quy trình ETL và Data Lake trên AWS là gì? Khám phá quy trình ETL và Data Lake trên AWS là một workshop tương tác và thực hành được thiết kế để hướng dẫn bạn qua các quy trình chính trong lĩnh vực kỹ thuật dữ liệu. Bạn sẽ đi sâu vào các giai đoạn quan trọng như nhập dữ liệu, cấp nước, khám phá, và tiêu thụ trong một data lake trên AWS.\n🎯 Lợi ích của Khám phá quy trình ETL và Data Lake trên AWS Trải nghiệm này cung cấp cho bạn kinh nghiệm thực tế về cách sử dụng dịch vụ phân tích AWS để xây dựng một data lake. Khi tham gia, bạn sẽ có cơ hội thực hành với các công cụ và dịch vụ sau:\nDịch vụ Amazon Kinesis\nThu thập và phân tích dữ liệu luồng theo thời gian thực.\nDịch vụ Di chuyển Dữ liệu AWS\nNhập dữ liệu theo lô vào data lake của bạn một cách hiệu quả.\nAWS Glue\nQuản lý và danh mục hóa dữ liệu cũng như thực hiện các quá trình ETL.\nAmazon Athena\nTruy vấn dữ liệu lớn trực tiếp trong data lake của bạn với cú pháp giống SQL.\nAmazon QuickSight\nSử dụng trực quan hóa dữ liệu mạnh mẽ để khám phá thông tin từ dữ liệu của bạn.\nVào cuối workshop này, bạn sẽ có được các kỹ năng thực tế để xây dựng một data lake trên AWS, nền tảng đám mây, bảo mật và không máy chủ!\n🚀 Sẵn sàng bắt đầu chưa? Hãy bắt đầu nào!\n"
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]