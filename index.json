[
{
	"uri": "/4-transforming-glue/1-etl/",
	"title": "Data Validation and ETL (1)",
	"tags": [],
	"description": "",
	"content": "üõ† PART A: Create Glue Crawler for Initial Full Load Data 1. Navigate to the AWS Glue Console\nOpen the AWS Glue Console in your browser. 2. Select Crawlers\nFrom the AWS Glue menu, under Data Catalog, select Crawlers. 3. Create Crawler: Click on the Create crawler button.\n4. Enter Crawler Name\nEnter glue-lab-crawler as the crawler name for the initial data load. Optionally, add a description that is descriptive and easy to recognize. Click Next. 5. Choose Data Sources and Classifiers\nUnder Choose data sources and classifiers, select Not yet. Click Add a data source. 6. Add a Data Source\nOn the Add a data store page, make the following selections: For Data source, select S3 from the drop-down box. For Location of S3 data, choose In this account. For S3 path, browse to the target folder where CSV files are stored. Example: s3://xxx-dmslabs3bucket-xxx/tickets/ Leave all other parameters as default. Click Add an S3 data source. 7. Click Next: Click Next to proceed.\n8. Configure Security Settings\nOn the Configure security settings page, select: Under Existing IAM role, choose \u0026lt;stackname\u0026gt;-GlueLabRole-\u0026lt;RandomString\u0026gt;, which has been pre-created for you.\nFor example: mod-6fccddd126456789-GlueLabRole-ZOQDII7JTBUM Click Next. 9. Set Output and Scheduling\nUnder Target database, click Add database, which will open a new tab. Enter ticketdata as the database name and click Create database. 10. Return to Set Output and Scheduling Page\nAfter creating the database, return to the Set output and scheduling page: Select Target database as ticketdata.\n(Use the refresh option next to the dropdown to ensure that the new database is listed.) Leave the Prefix added to tables field empty (optional). For Crawler schedule, select On demand as the frequency. Click Next. 11. Review and Create the Crawler\nReview the information, and click Create crawler. The crawler is now created and ready to run. 12. Run the Crawler\nClick Run crawler to execute the crawler. The status will change from Running to Stopping. Wait until the status returns to Ready (this process may take a few minutes). After completion, the crawler will create 15 tables. üöÄ Congratulations! You\u0026rsquo;ve successfully created and executed a Glue Crawler!\nüõ† PART B: Data Validation Exercise 1. Open the Person Table\nWithin the Tables section of your ticketdata database, click on the person table. You may notice that some tables, like the person table, have default column headers such as col0, col1, col2, col3. These default names are used when headers are absent or the crawler cannot determine them. 2. Edit the Schema\nClick on the Edit Schema option. 3. Modify Column Names\nIn the Edit Schema section, locate the row for col0 (column name). Click on Edit next to col0. Type id as the new column name and click Save. Repeat this process to rename the remaining columns to the following names:\ncol1 ‚Üí full_name col2 ‚Üí last_name col3 ‚Üí first_name 4. Save the Changes\nOnce all column names have been updated, click Save as new table version to apply the changes. "
},
{
	"uri": "/2-clickstream-anomaly-detection-with-flink/1-prelab-setup/",
	"title": "Prelab Setup",
	"tags": [],
	"description": "",
	"content": "üåü Introduction Welcome to the Real-Time Clickstream Anomaly Detection pre-lab! This guide will help you set up the environment to detect anomalies in real-time using Amazon Managed Service for Apache Flink. By the end of the pre-lab, you will have access to the following resources:\nTwo Amazon S3 Buckets to store raw and processed data. A Lambda function that triggers when anomalies are detected. An SNS Topic that sends notifications to your email/phone when anomalies occur. Amazon Cognito User Credentials for logging into the Kinesis Data Generator to send data. üõ†Ô∏è CloudFormation Stack Deployment Step 1: Selecting the Region AWS Event: Your instructor will inform you of the region to use. Own Account: Choose your AWS region (e.g., US-EAST-1 (N. Virginia)) and stick to it for the entire workshop. Step 2: Deploying the CloudFormation Template Click the Deploy to AWS button below to set up the necessary infrastructure!\nDeploy to AWS\nStep 3: Fill in the Parameters: Username: Choose a unique username (e.g., test). Password: Create a password with at least 6 characters, including a number and a capital letter. Email: Provide a valid email to receive notifications from SNS. SMS: Enter your phone number for text notifications (e.g., +1XXXXXXXXX). Important: Don\u0026rsquo;t forget to acknowledge that AWS CloudFormation will create IAM resources by ticking the checkbox.\nStep 4: Click Create! After a few minutes, check the CREATE_COMPLETE status in the CloudFormation dashboard.\nüìß Confirm Subscription (Email \u0026amp; SMS) Check Your Email:\nLook for a subscription confirmation email from SNS. If you don‚Äôt see it, check the spam folder! Click the Confirmation Link:\nConfirm the subscription and get ready for notifications when anomalies are detected. Once your stack is deployed, click the Outputs tab to view more information: KinesisDataGeneratorUrl - This value is the Kinesis Data Generator (KDG) URL. RawBucketName - Name of bucket to store the raw data coming from KDG. ProcessedBucketName - Name of the bucket to store transformed data\nüèóÔ∏è Set up the Amazon Kinesis Data Generator (KDG) Login to KDG\nGo to the Outputs tab in CloudFormation and click the KinesisDataGeneratorUrl.\nSign in with the username and password you created during deployment. Create Templates for Clickstream Payloads\nSet up the following payload templates in the KDG:\nSchema Discovery Payload: {\u0026#34;browseraction\u0026#34;:\u0026#34;DiscoveryKinesisTest\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Click Payload: {\u0026#34;browseraction\u0026#34;:\u0026#34;Click\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Impression Payload: {\u0026#34;browseraction\u0026#34;:\u0026#34;Impression\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;yourwebsiteurl.domain.com\u0026#34;} Note: Do NOT send the data just yet ‚Äî we\u0026rsquo;ll do that during the main lab. The templates are now ready!\nüì≤ Verify Your Email and SMS Subscription Go to SNS Topics\nIn the SNS console, select Topics. Select ClickStreamEvent Topic\nYou should see your email and phone number listed under subscriptions. If the status is pending, click to confirm your subscription. Confirmation:\nMake sure you confirm via the email link to start receiving notifications. üßë‚Äçüíª Observe AWS Lambda Anomaly Function The CloudFormation template has already deployed a Lambda function that will trigger whenever an anomaly is detected.\nGo to Lambda Console\nNavigate to AWS Lambda and find CSEBeconAnomalyResponse under Functions. Review Lambda Code\nScroll down to the code section. You will see that the TopicArn value matches your email/SMS subscription. Lambda will send notifications to this topic once an anomaly occurs. üéâ Congratulations! You\u0026rsquo;ve Completed the Prelab! You now have everything you need to dive into the main lab! You‚Äôll be able to detect and handle real-time anomalies in clickstream data using Amazon Managed Service for Apache Flink.\nGood luck and enjoy the lab! üéâüéâüéâ\n"
},
{
	"uri": "/1-introduction/1-local-system-setup/",
	"title": "Local System Setup",
	"tags": [],
	"description": "",
	"content": "For the hands-on part of this workshop, you‚Äôll need a laptop and a web browser.\nüåê Browser Requirements Ensure your laptop is running the latest version of one of the following web browsers:\n‚úÖ Mozilla Firefox\n‚úÖ Google Chrome\n‚úÖ Microsoft Edge\nHaving an up-to-date browser ensures compatibility with AWS services and a smooth experience throughout the workshop.\nüí° Tip: If you experience any issues, try clearing your browser cache or switching to a different browser.\nüöÄ Once your system is ready, proceed to the next step!\n"
},
{
	"uri": "/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Welcome to this exciting hands-on lab series on ETL \u0026amp; Data Lake Immersion! üöÄ These labs are structured to guide you step by step through essential data management and analysis processes using AWS.\nüåü What You Will Learn Throughout this workshop, you‚Äôll master:\n‚úÖ Data Ingestion ‚Äì Efficiently load and process raw data.\n‚úÖ Data Transformations ‚Äì Clean, enrich, and structure data for analysis.\n‚úÖ Exploring the Data Lake ‚Äì Use SQL queries and visualization tools for insights.\nüèóÔ∏è Architecture Patterns üì• 1. Perform Data Ingestion\n‚öôÔ∏è 2. Execute Data Transformations\nüåä 3. Explore the Data Lake\nüõ†Ô∏è Setup Requirements üí° Before You Begin:\nEnsure you have full access to the AWS Console. Follow the setup guides below to configure your Local System, AWS Account, and IAM User. If you‚Äôre participating in an AWS-hosted event, additional instructions will be provided.\nüîπ Step-by-Step Setup 1Ô∏è‚É£ Local System Setup ‚Äì Get your local environment ready.\n2Ô∏è‚É£ AWS Account and IAM User ‚Äì Set up AWS credentials and permissions.\n3Ô∏è‚É£ Workshop at an AWS Event ‚Äì Follow instructions for AWS-hosted sessions.\nüöÄ Get Started This workshop follows a guided, step-by-step approach, complete with hints to help you troubleshoot any issues. If you have questions, our lab instructors are here to assist you!\nüîπ Follow along, experiment, and learn by doing.\nüîπ Ask questions and explore different approaches.\nüîπ Have fun! Learning data engineering should be exciting!\nüî• Let‚Äôs get started! üöÄ\n"
},
{
	"uri": "/4-transforming-glue/2-etl-2/",
	"title": "Data Validation and ETL (2)",
	"tags": [],
	"description": "",
	"content": "üõ† PART C: Data ETL Exercise Steps for Creating the ETL Job 1. Navigate to ETL Jobs :In the left navigation pane, click on ETL jobs. 2. Choose Visual ETL\nClick on Visual ETL. 3. Select S3 Data Source\nFrom the Sources list, select Amazon S3 to add a Data source - S3 bucket node. 4. Show Data Source Properties\nSelect the Data source - S3 bucket node to view the data source properties. 5. Configure Data Source Properties\nIn the panel on the right, under Data source properties - S3, select Data Catalog table. Choose the ticketdata database from the drop-down. For Table, select the sport_team table. 6. Add Change Schema Transform\nClick on the + button and select Change Schema from the Transforms list to add a Transform - Change Schema node. 7. Modify Column Data Type\nSelect the Transform - Change Schema node to view the properties. In the Transform panel on the right, change the data type of the id column to double from the dropdown. 8. Add S3 Data Target\nClick on the + button again and select Amazon S3 from the Targets list to add a Data target - S3 bucket node. Select the Data target - S3 bucket node to view the properties. In the panel on the right: Change the Format to Parquet from the dropdown. Under Compression Type, select Uncompressed from the dropdown. Under S3 Target Location, select Browse S3. Browse to the mod-xxx-dmslabs3bucket-xxx bucket, select the tickets item, and press Choose. In the text box, append dms_parquet/sport_team/ to the S3 URL. The path should look like: s3://mod-xxx-dmslabs3bucket-xxx/tickets/dms_parquet/sport_team/ Don\u0026rsquo;t forget the / at the end. The job will automatically create the folder. 9. Configure Job Details\nSelect the Job details tab at the top. Enter Glue-Lab-SportTeamParquet as the Name. For IAM Role, select the role named similar to mod-xxx-GlueLabRole-xxx. Scroll down the page, and under Job bookmark, select Disable from the dropdown. (You can try out the bookmark functionality later in this lab.) 10. Save the Job: Press the Save button in the top right corner to create the job. 11 . Run the Job: After the job is created successfully, click the Run button to start the job.\n15. Monitor the Job\nSelect Jobs from the left-hand navigation panel to view a list of your jobs. Click on Monitoring to view the status of running jobs, success/failure rates, and other statistics. 16. Verify Job Completion\nScroll down to the Job runs list to verify that the ETL job has completed successfully. This process should take about 1 minute. Repeat for Other Tables You need to repeat this process for an additional 4 jobs to transform the sport_location, sporting_event, sporting_event_ticket, and person tables. During this process, modify the column data types as required.\nYou can either:\nRepeat the process above for each table. Clone the first job and update the details for each table. üõ† PART D: Create Glue Crawler for Parquet Files 1. Navigate to Crawlers in Glue\nIn the Glue navigation menu, under Data Catalog, select Crawlers. Click Create crawler. 2. Provide Crawler Name\nFor the Crawler name, type glue-lab-parquet-crawler and click Next. 3. Add Data Store\nIn the Add data store screen: For Choose a data store, select S3. For Location of S3 data, select In this account. For S3 path, specify the S3 path (Parent Parquet folder) containing the nested Parquet files, e.g., s3://xxx-dmslabs3bucket-xxx/tickets/dms_parquet/ Leave all other parameters as default. Click Add an S3 data source. 4. Choose Data Source and Classifiers\nIn the Choose data sources and classifiers screen, click Next. 5. Configure Security Settings\nOn the Configure security settings page, select Choose an existing IAM role. For IAM role, select the existing role xxx-GlueLabRole-xxx. Click Next. 6. Set Output and Scheduling\nOn the Set output and scheduling page: Under Target database, choose the existing database you created earlier, e.g., ticketdata. For Prefix added to tables (optional), type parquet_. For Crawler schedule, select On Demand. Click Next. 7. Review and Create Crawler\nReview the summary and click Create crawler. 9. Run the Crawler\nAfter creating the crawler, click Run Crawler. Once the crawler finishes running, you should see that the tables were added, depending on how many Parquet ETL conversions you set up in the previous section. 10. Verify Tables\nIn the left navigation pane, click Tables. Add the filter parquet for Classification to return the newly created tables. üéâ Congratulations! You\u0026rsquo;ve successfully completed the Data Validation and ETL lab.\n"
},
{
	"uri": "/2-clickstream-anomaly-detection-with-flink/2-real-time-clickstream-anomaly-detection/",
	"title": "Lab: Real Time Clickstream Anomaly Detection",
	"tags": [],
	"description": "",
	"content": "Introduction This guide helps you complete Real-Time Clickstream Anomaly Detection using Amazon Managed Service for Apache Flink.\nAnalyzing web log traffic to gain insights that drive business decisions has historically been performed using batch processing. Although effective, this approach results in delayed responses to emerging trends and user activities.\nThere are solutions that process data in real-time using streaming and micro-batching technologies, but they can be complex to set up and maintain.\nAmazon Managed Service for Apache Flink is a managed service that makes it easy to identify and respond to changes in data behavior in real-time.\nSteps ‚úÖ Set up an Amazon Analytics Studio Application through CloudFormation stack deployment ‚úÖ Generate real-time website traffic using Amazon Kinesis Data Generator (KDG) ‚úÖ Perform real-time Data Analytics ‚úÖ Environment Cleanup ‚úÖ Appendix: Anomaly Detection Scripts In the Kinesis prelab setup, you fulfilled the prerequisites for this lab. Now, you will create an Amazon Managed Service for Apache Flink pipeline.\n1Ô∏è‚É£ Set up an Amazon Analytics Studio Application through CloudFormation stack deployment Make sure you are in the appropriate AWS region. Click the Deploy to AWS link here to stand up the pre-lab workshop infrastructure. The button above will open a ‚ÄúQuick create stack‚Äù form. ‚úÖ Accept the default parameters ‚úÖ Select the checkbox to acknowledge new IAM role creation ‚úÖ Click Create Stack to run the Amazon CloudFormation Template The stack will create six Amazon Kinesis Data Streams in the Amazon Kinesis Console: tickerstream ‚Äì the raw stream to send the initial traffic clickstream ‚Äì captures the number of clicks impressionstream ‚Äì captures the number of impressions ctrstream ‚Äì captures the calculated click-through rate destinationstream ‚Äì captures the anomaly scores anomalydetectionstream ‚Äì captures the records with anomaly scores greater than 2 The template will also create an Amazon Analytics Studio application called\nkda-flink-prelab-RealtimeApplicationNotebook in the Amazon Kinesis Application Console ‚Üí Studio tab. Run the Studio Application: Select kda-flink-prelab-RealtimeApplicationNotebook under the Studio tab. Select ‚ÄúRun‚Äù again on the next screen. 2Ô∏è‚É£ Generate real-time website traffic using Amazon Kinesis Data Generator (KDG) Navigate to the Amazon CloudFormation Console in your AWS account. Click on the Kinesis-pre-lab stack created during the Streaming Data Analytics Prelab setup. Go to the Outputs tab to get the Kinesis Data Generator link.\nOpen two concurrent sessions of the KDG UI in your browser.\nSign in using the username and password entered in the CloudFormation template.\nSession 1: Send impression messages at a rate of 1 message per second for 30 seconds to the tickerstream. Message body: {\u0026#34;browseraction\u0026#34;: \u0026#34;Impression\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;https://www.mysite.com\u0026#34;} Session 2: Send click messages at a rate of 5 messages per second for 30 seconds to the tickerstream. Message body: {\u0026#34;browseraction\u0026#34;: \u0026#34;Click\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;https://www.mysite.com\u0026#34;} View the number of messages being sent to the data stream. Stop sending messages after 30 seconds. 3Ô∏è‚É£ Perform real-time Data Analytics Navigate to the Amazon Kinesis Application Console. Under the Studio tab, select kda-prelab-template-RealtimeApplicationNotebook. Select \u0026ldquo;Open in Apache Zeppelin\u0026rdquo;. In the Apache Zeppelin Console, select \u0026ldquo;Create new note\u0026rdquo;. Provide the notebook name as kda_Interactive_notebook. Now let\u0026rsquo;s perform real-time interactive analytics with Kinesis data streams: ‚úîÔ∏è Create Flink tables using Flink SQL Queries\n‚úîÔ∏è Use Flink SQL queries to transform and create new data streams in real-time\n‚úîÔ∏è Perform anomaly detection using Flink User Defined Function (UDF) and trigger anomaly notification emails in real-time\nSteps Run the table creation scripts. Run the User Defined Function (UDF) for Anomaly Detection using Random Cut Forest algorithm. View real-time website visit data. Filter impressionstream from tickerstream. Filter clickstream from tickerstream. Calculate Click-Through Rate (CTR) and populate ctrstream. View the Click-Through Rate in real-time. Use UDF (Random Cut Forest) to generate anomaly scores. Populate anomalydetectionstream. Check anomaly scores from Random Cut Forest algorithm in real-time. Receive email notifications when anomalies are detected. Troubleshooting (If no anomaly emails are received) Open two concurrent sessions of KDG UI again. Repeat steps 2 - 10. You should start receiving email notifications from the second attempt. 4Ô∏è‚É£ Environment Clean Up After completing the lab, stop your application: Click Actions ‚Üí Stop Application to avoid a flood of SMS and email messages. To delete the entire resource stack: Navigate to Amazon CloudFormation Console ‚Üí Stacks Select kda-flink-pre-lab and click Delete. Select Delete on the next screen. If the Delete Stack operation fails due to non-empty S3 buckets: Select Delete again. In the pop-up, either retain the S3 buckets or manually empty them. Click Delete again, and this time, the stack should be removed completely. üìå Appendix: Anomaly Detection Scripts Amazon CloudFormation Template:\nüìå 1-Lab-Kinesis-Clickstream_CFN.yaml Apache Zeppelin Notebook:\nüìå kda_realtime_interactive_streaming_notebook.zpln Amazon Random Cut Forest User Defined Function:\nüìå RandomCutForestUDF.java üéâ Congratulations! You have successfully completed the Real-Time Clickstream Anomaly Detection Lab! üöÄ\n"
},
{
	"uri": "/1-introduction/2-aws-account-setup/",
	"title": "AWS Account and IAM User",
	"tags": [],
	"description": "",
	"content": "To get started with AWS services, you first need an AWS account and an IAM (Identity and Access Management) user with administrator privileges. Here\u0026rsquo;s everything you need to know:\nüõ°Ô∏è AWS Identity and Access Management (IAM) AWS IAM allows you to securely manage access to AWS services and resources. With IAM, you can create and manage users, groups, and assign permissions to allow or deny access to specific AWS resources.\nBYOA - Bring Your Own Account You will typically need to bring your own AWS account with an IAM user that has administrator privileges. If possible, it\u0026rsquo;s best to use a clean or \u0026ldquo;vanilla\u0026rdquo; account.\nImportant: When using your own AWS account (personal or organizational), make sure you understand the policies and implications regarding resource provisioning.\nIf you don‚Äôt have an AWS account yet and want to follow along with this workshop, you can create a free account here, or ask your Cloud Center of Excellence (CCoE) to set one up for you.\nüßë‚Äçüíª Creating an Administrator IAM User If you already have an AWS account but no IAM user with administrator access, follow these steps to create one:\nCreate an AWS account if you don‚Äôt have one already, by clicking here.\nOnce your account is ready, create a new IAM user for this workshop by following these steps:\nEnter the user details and select the \u0026ldquo;Programmatic access\u0026rdquo; option for access type. Attach the AdministratorAccess IAM policy: This grants full administrative access to your AWS resources. If needed, you can choose fine-grained access for specific Amazon services used in this workshop. Click Next ‚Üí Ignore Tags ‚Üí Next Review. Click Create User. Download the CSV file containing the Access Key ID and Secret Key for future use. üíª Installing AWS Command Line Interface (CLI) on Your Local Machine To install the AWS CLI, follow the official installation guide in the AWS CLI documentation.\nOnce installed, link the AWS CLI to your AWS account using the IAM credentials (Access Key and Secret Key) that you created earlier:\nOpen your terminal and run the following command: aws configure Enter the following details when prompted: AWS Access Key ID (from your CSV file). AWS Secret Access Key (from your CSV file). Default Region Name (e.g., us-east-1). Leave Default Output Format empty, or select a format like json. üéâ You‚Äôre Ready! Now that the AWS CLI is set up and configured, you\u0026rsquo;re ready to proceed with the rest of the workshop! üöÄ\nüìö Additional Resources AWS IAM Documentation AWS CLI Documentation "
},
{
	"uri": "/2-clickstream-anomaly-detection-with-flink/",
	"title": "Lab: Real-Time Clickstream Anomaly Detection with Amazon Managed Service for Apache Flink",
	"tags": [],
	"description": "",
	"content": "üåü Introduction Streaming data is everywhere! It‚Äôs continuously generated by thousands of sources, arriving in small chunks (kilobytes) and processed in real time. Think of:\nüì± Mobile \u0026amp; Web App Logs | üõí E-commerce Transactions | üéÆ In-Game Player Activity\nüìä Financial Trading Data | üåç Geospatial Tracking | üì° IoT \u0026amp; Telemetry Data\nTo make sense of this fast-moving data, businesses analyze it on the fly, detecting trends, anomalies, and insights. Companies use streaming analytics for:\n‚úÖ Billing \u0026amp; Metering ‚Äì Track usage in real time\n‚úÖ Server \u0026amp; App Monitoring ‚Äì Detect system failures before they escalate\n‚úÖ Clickstream Analysis ‚Äì Understand user behavior instantly\n‚úÖ Social Media Sentiment ‚Äì Respond to trends as they happen\nüèóÔ∏è AWS Streaming Stack Amazon Web Services (AWS) provides powerful tools for working with streaming data:\nüîπ Amazon Kinesis Data Streams ‚Äì Capture real-time data\nüîπ Kinesis Data Firehose ‚Äì Stream and load data to storage or analytics\nüîπ Amazon Managed Service for Apache Flink ‚Äì Real-time processing \u0026amp; anomaly detection\nüîπ Managed Streaming for Kafka (MSK) ‚Äì Stream and analyze data efficiently\nüõ†Ô∏è Lab Overview In this hands-on lab, you‚Äôll work with AWS services to build a real-time anomaly detection system for clickstream data.\nüîπ Lab Tasks 1Ô∏è‚É£ Prelab Setup ‚Äì Get your AWS environment ready.\n2Ô∏è‚É£ Real-Time Clickstream Anomaly Detection ‚Äì Use Apache Flink to detect anomalies in real-time.\n3Ô∏è‚É£ Streaming ETL with AWS Glue ‚Äì Transform streaming data efficiently.\nüéØ What You‚Äôll Learn ‚úÖ How to ingest, process, and analyze streaming data\n‚úÖ How to use Apache Flink, Kinesis, and MSK for real-time processing\n‚úÖ How to detect anomalies in clickstream data\n‚úÖ How to set up and manage AWS services for streaming analytics\nüí° Ready to dive in? Let‚Äôs get started! üöÄ\n"
},
{
	"uri": "/3-ingestion-with-dms/",
	"title": "Ingestion With DMS: Skip DMS Lab",
	"tags": [],
	"description": "",
	"content": "Introduction üìå What is AWS DMS? AWS Database Migration Service (AWS DMS) enables you to migrate databases to AWS quickly and securely, with minimal downtime to your applications. The source database remains fully operational throughout the migration process, ensuring business continuity.\nüîÑ Migration Types Supported AWS DMS supports both homogeneous and heterogeneous database migrations:\nHomogeneous Migration (Same database engines)\nExample: Oracle ‚ûù Oracle, MySQL ‚ûù MySQL Heterogeneous Migration (Different database engines)\nExample: Oracle ‚ûù Amazon Aurora, SQL Server ‚ûù Amazon RDS for PostgreSQL ‚ö° Continuous Data Replication With low-latency replication, AWS DMS allows you to:\nStream data from multiple sources into Amazon S3 to build a highly available data lake. Consolidate multiple databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. üèÜ Option 3: Skip DMS Lab Steps Overview Open AWS CloudShell Copy data from the staging Amazon S3 bucket to your own S3 bucket Verify the data in Amazon S3 About the Lab Setup The source database is an RDS PostgreSQL instance storing ticket sales for sporting events. AWS DMS (Database Migration Service) is typically used to migrate data from RDS to an S3 bucket. Instead of using DMS, you can directly copy the source data to your S3 bucket and proceed with the lab. In this lab, you will: ‚úÖ Copy data from a centralized S3 bucket\n‚úÖ Use an AWS Glue Crawler to catalog metadata\n‚úÖ Transform the data with AWS Glue\n‚úÖ Query \u0026amp; create a View in Athena\n‚úÖ Build a dashboard with Amazon QuickSight Step 1: Open AWS CloudShell Launch AWS CloudShell in your AWS account. If a pop-up appears, close it. AWS CloudShell is region-specific, check AWS Regional Services List for availability. Step 2: Copy Data from the Staging S3 Bucket Run the following command in CloudShell, replacing \u0026lt;YourBucketName\u0026gt; with your actual S3 bucket name:\nexport dmslabs3bucket=\u0026lt;YourBucketName\u0026gt; aws s3 cp s3://ws-assets-prod-iad-r-iad-ed304a55c2ca1aee/976050cc-0606-4b23-b49f-ca7b8ac4b153/v1/data/pg-source.zip /tmp \u0026amp;\u0026amp; unzip /tmp/pg-source.zip -d /tmp/ \u0026amp;\u0026amp; aws s3 cp --recursive --copy-props none /tmp/dms_sample s3://$dmslabs3bucket/tickets/dms_sample Verify the Data in S3 1. Access the S3 Console\nNavigate to the AWS S3 Console and locate the data copied through CloudShell terminal. 2. Find Your S3 Bucket\nYour S3 bucket name follows this format: BucketName/bucket_folder_name/schema_name/table_name/objects/ Example from our lab: /\u0026lt;BucketName\u0026gt;/tickets/dms_sample Each table_name has its own separate path. 3. Review a File Using S3 Select\nNavigate to the player directory. Select the checkbox next to a file name. Click the Actions dropdown button and choose Query with S3 Select. On the Query with S3 Select page: Leave the default values for Input Settings and SQL Query. Click Run SQL query. "
},
{
	"uri": "/2-clickstream-anomaly-detection-with-flink/3-streaming-etl-with-aws-glue/",
	"title": "Lab: Streaming ETL With Aws Glue",
	"tags": [],
	"description": "",
	"content": "üéì Introduction In this lab, you will learn how to ingest, process, and consume streaming data using AWS serverless services such as:Kinesis Data Streams , Glue, S3, Athena\nTo simulate the data streaming input, we will be using the Kinesis Data Generator (KDG).\n‚ö° Set Up the Kinesis Stream Navigate to the AWS Kinesis console. Click Create data stream.\nFill in the details:\nStream name: TicketTransactionStreamingData Capacity mode: Provisioned Provisioned shards: 2 Click Create data stream. ‚úîÔ∏è üìù Lab Instructions üîë Create Table for Kinesis Stream Source in Glue Data Catalog Navigate to the AWS Glue console. In the AWS Glue menu, select Data Catalog ‚Üí Tables and click the Add table button. Enter TicketTransactionStreamData as the table name. Click Create database, name it tickettransactiondatabase, and click Create. From the dropdown, select the tickettransactiondatabase and click Next. Choose Kinesis as the source, then: Select the appropriate AWS region. Choose TicketTransactionStreamingData from the stream dropdown. Set JSON as the incoming data format. Leave the schema empty (we\u0026rsquo;ll enable schema detection in the next step) and click Next. Review the settings and click Create. Click on the table to view its properties. üíº Create and Trigger the Glue Streaming Job In the Glue Console, click Data Integration ‚Üí ETL jobs. Click Create job from a blank graph. For the source, select Amazon Kinesis. Click on the Amazon Kinesis node on the canvas, then configure as follows: Name: Amazon Kinesis Source: Data Catalog table Database: tickettransactiondatabase Table: tickettransactionstreamdata Ensure Detect schema is checked. Leave other fields at their defaults. Click the + button to add a node. Then, select Amazon S3 under the Targets tab. Click the Data target - S3 bucket on the canvas, and configure as follows: Format: Parquet Compression Type: Uncompressed S3 Target Location: Browse S3, select the bucket with dmslabs3bucket in the name, and append TicketTransactionStreamingData/ to the S3 path. The path should look like: s3://xxx-xxx-dmslabs3bucket-xxx/TicketTransactionStreamingData/ (ensure the / is included). Under the Job details tab, configure the following: Name: TicketTransactionStreamingJob IAM Role: Select xxx-GlueLabRole-xxx from the dropdown. Type: Spark Streaming Press Save to create the job. Once the job is created successfully, click Run to start processing the data. üîÑ üé• Trigger Streaming Data from KDG Launch KDG\nOpen KDG using the URL you bookmarked from the lab setup. Login using the user and password you provided when deploying the CloudFormation stack. Select Region and Kinesis Stream\nMake sure you select the appropriate region. From the dropdown list, select TicketTransactionStreamingData as the target Kinesis stream. Leave Records per second at the default value (100 records per second). For the record template, type in NormalTransaction as the payload name, and copy the template payload as follows: { \u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;, \u0026#34;transactionAmount\u0026#34;: {{random.number( { \u0026#34;min\u0026#34;:10, \u0026#34;max\u0026#34;:150 } )}}, \u0026#34;sourceIp\u0026#34; : \u0026#34;{{internet.ip}}\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({ \u0026#34;weights\u0026#34; : [0.8,0.1,0.1], \u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;] } )}}\u0026#34;, \u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Send Data: Click Send data to trigger the simulated transaction streaming data. üöÄ üõ†Ô∏è Create Glue Crawler for the Transformed Data Navigate to the AWS Glue console. In the AWS Glue menu, select Crawlers and click Create crawler. Name the crawler as TicketTransactionParquetDataCrawler and click Next. Click on Add a datasource. Choose S3 as the data store, and click Browse S3. Select the bucket containing dmslabs3bucket in the name. Choose the folder TicketTransactionStreamingData and click Choose. Once the S3 bucket and data path are configured, review the settings and click Add an S3 data source. As no other data sources are needed, click Next.\nFrom the dropdown, choose an existing IAM role with GlueLabRole in its name, then click Next. Select tickettransactiondatabase as the output database.\nSet parquet_ as the prefix for the table. Click Next. Set the crawler to run every hour to ensure newly added partitions are included. Review the crawler configuration and click Create crawler. Once the crawler is created, select the crawler and click Run crawler to trigger the first run. Wait for the crawler job to complete (status will show as \u0026ldquo;Completed\u0026rdquo;). üö®Trigger Abnormal Transaction Data from KDG Keep KDG streaming data running: Open another browser window and launch KDG using the URL you bookmarked from the lab setup. Login with the user and password you provided when deploying the Kinesis PreLab CloudFormation stack. Set up streaming configuration: Ensure you select the appropriate region. From the dropdown list, select the TicketTransactionStreamingData stream as the target Kinesis stream. Set Records per second to 1 (to simulate one transaction per second). Prepare the abnormal transaction payload:\nSelect Template 2 for the record template. Set the payload name to AbnormalTransaction. Copy the following payload template into the editor: { \u0026#34;customerId\u0026#34;: \u0026#34;{{random.number(50)}}\u0026#34;, \u0026#34;transactionAmount\u0026#34;: {{random.number( { \u0026#34;min\u0026#34;:10, \u0026#34;max\u0026#34;:150 } )}}, \u0026#34;sourceIp\u0026#34; : \u0026#34;221.233.116.256\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;{{random.weightedArrayElement({ \u0026#34;weights\u0026#34; : [0.8,0.1,0.1], \u0026#34;data\u0026#34;: [\u0026#34;OK\u0026#34;,\u0026#34;FAIL\u0026#34;,\u0026#34;PENDING\u0026#34;] } )}}\u0026#34;, \u0026#34;transactionTime\u0026#34;: \u0026#34;{{date.now}}\u0026#34; } Click \u0026ldquo;Send data\u0026rdquo; to trigger the abnormal transactions (1 transaction per second, all from the same source IP address). üîç Detect Abnormal Transactions using Ad-Hoc Query from Athena Navigate to AWS Athena console:\nIf it‚Äôs your first time using Athena, expand the Get Started using Athena section below. Select Data source and database:\nChoose AwsDataCatalog as the data source. Select tickettransactiondatabase as the database. Refresh to ensure that the parquet_tickettransactionstreamingdata table is showing in the list. Query the number of transactions by source IP: Copy the following query to count the transactions from each source IP in the last hour: SELECT count(*) as numberOfTransactions, sourceip FROM \u0026#34;tickettransactiondatabase\u0026#34;.\u0026#34;parquet_tickettransactionstreamingdata\u0026#34; WHERE ingest_year=\u0026#39;2024\u0026#39; AND cast(ingest_year as bigint)=year(now()) AND cast(ingest_month as bigint)=month(now()) AND cast(ingest_day as bigint)=day_of_month(now()) AND cast(ingest_hour as bigint)=hour(now()) GROUP BY sourceip ORDER BY numberOfTransactions DESC; Execute the query. You should see a large amount of transactions coming from the same source IP. "
},
{
	"uri": "/4-transforming-glue/",
	"title": "Transforming data with Glue",
	"tags": [],
	"description": "",
	"content": "üéØ Introduction In this lab, you will explore AWS Glue, a powerful serverless data integration service designed to simplify the process of discovering, preparing, moving, and integrating data from various sources for analytics, machine learning (ML), and application development.\nAWS Glue Crawlers: These crawlers help populate the AWS Glue Data Catalog with tables, which is the primary method used by most users. The crawler can crawl multiple data stores in a single run. Data Catalog: Upon completion, the crawler creates or updates tables in your Data Catalog. ETL Jobs: The Extract, Transform, and Load (ETL) jobs that you define in AWS Glue use these Data Catalog tables as sources and targets. These jobs will read from and write to the data stores specified in the source and target tables. üõ† Prerequisites Completion of the Ingestion with DMS Lab (any option) is required before starting this lab. üìã Summary of Tasks In this lab, you will complete the following tasks. You can choose to complete only Data Validation and ETL to progress to the next lab, where you will query tables using Amazon Athena and visualize them with Amazon QuickSight.\nTasks: Data Validation and ETL (1) Data Validation and ETL (2) "
},
{
	"uri": "/5-query-and-visualize/",
	"title": "Athena and Quicksight",
	"tags": [],
	"description": "",
	"content": "üîç Query Data with Amazon Athena Step 1: Open Amazon Athena In the AWS Services Console, search for Athena. If this is your first time using Athena in your AWS Account, expand \u0026ldquo;Get Started using Athena\u0026rdquo; and follow the setup instructions. Step 2: Select Database and Inspect Tables In the Query Editor, select your newly created database (e.g., \u0026quot;ticketdata\u0026quot;). Click on the table \u0026ldquo;parquet_sporting_event_ticket\u0026rdquo; to inspect its fields. The data types for id, sporting_event_id, and ticketholder_id should be (double). Step 3: Query Across Multiple Tables Next, we will query across the tables:\nparquet_sporting_event parquet_sport_team parquet_sport_location üìå Copy and run the following SQL query in the Athena Query Editor:\nSELECT e.id AS event_id, e.sport_type_name AS sport, e.start_date_time AS event_date_time, h.name AS home_team, a.name AS away_team, l.name AS location, l.city FROM parquet_sporting_event e, parquet_sport_team h, parquet_sport_team a, parquet_sport_location l WHERE e.home_team_id = h.id AND e.away_team_id = a.id AND e.location_id = l.id; Step 4: Create a View Click Create and then select View from query. Name the view sporting_event_info and click Create. Your new view is now created. Step 5: Query the View Open a new query tab in Athena. Copy and paste the following SQL query: SELECT t.id AS ticket_id, e.event_id, e.sport, e.event_date_time, e.home_team, e.away_team, e.location, e.city, t.seat_level, t.seat_section, t.seat_row, t.seat, t.ticket_price, p.full_name AS ticketholder FROM sporting_event_info e, parquet_sporting_event_ticket t, parquet_person p WHERE t.sporting_event_id = e.event_id AND t.ticketholder_id = p.id; Step 7: Save the Query Click on Save as. Give this query a name: create_view_sporting_event_ticket_info and some description. Click on Save query. Back to the query editor, you will see the query name changed. Now, click on Run. The results appear beneath the query window. Step 8: Create Another View Click View from query. Name the view sporting_event_ticket_info. Click Create. Step 9: Copy and Run the Query Copy the following SQL syntax into the Query 5 tab: SELECT sport, count(distinct location) as locations, count(distinct event_id) as events, count(*) as tickets, avg(ticket_price) as avg_ticket_price FROM sporting_event_ticket_info GROUP BY 1 ORDER BY 1; Click on Save as and give this query name: analytics_sporting_event_ticket_info and some description and then, click on Save. The result of queries Create QuickSight Charts This section guides you through different chart types in QuickSight.\nStep 1: Populate the Chart In the Fields list, click the ticket_price column to populate the chart. Click the expand icon in the corner of ticket_price field. Select Show as Currency to display the number in dollar value. Step 2: Add a Visual Click the Add button from the Visuals pane. In the Visual types area, choose the Vertical bar chart icon. Set the X-axis: In the Fields list, select event_date_time. The visualization should update automatically. Set the Y-axis (Value): Select ticket_price from the Fields list. Step 3: Adjust and Group Data Drag and move visuals to adjust space in the dashboard. In the Fields list, click and drag seat_level to the Group/Color box. Use the slider below the X-axis to fit all the data. Step 4: Change the Chart Type In the Visuals pane, click Vertical bar chart. Under Change visual type, choose the Clustered bar combo chart icon. In the Fields list, click and drag ticketholder to the Lines box. In the Lines box: Click the dropdown box. Choose Aggregate: Count Distinct for Aggregate. The Y-axis should update on the right-hand side. Step 5: Explore Insight Create QuickSight Parameters In this section, we will create parameters with controls for the dashboard and assign them to a filter for all visuals.\nStep 1: Open Parameters From the Toolbar, select the Parameters icon. Step 2: Create the EventFrom Parameter Click Add to create a new parameter. For Name, type EventFrom. For Data type, choose Datetime. For Time granularity, set Hour. For Default value, select a date from the calendar corresponding to the earliest event_date_time in your graph. Example: 2021-01-01 00:00. Click Create, then close the Parameter Added dialog box. Step 3: Create the EventTo Parameter Click Add to create another parameter. Set the following attributes: Name: EventTo Data type: Datetime Time granularity: Hour Default value: Select a date from the calendar corresponding to the latest event_date_time in your graph. Example: 2022-01-01 00:00. Click Create. Step 4: Add Controls In the next window, you can select any option to perform operations with the parameter. Alternatively, click the three dots for the EventFrom parameter and choose Add control. For Display name, specify Event From and click Add. Repeat the process to add a control for EventTo, setting the Display name as Event To. Now, you should see and expand the Controls section above the chart.\nCreate a QuickSight Filter To finalize the process, we will connect a filter to the controls for all visuals.\nFrom the Toolbar, choose Filter. Click the plus icon (+) to add a filter for the field event_date_time. Click the newly added filter to edit its properties. Set Applied To dropdown to All applicable visuals. \\ For Filter type, choose Date \u0026amp; Time range. Set Condition as Between. Select Use Parameter, click Yes to apply to all visuals. Set Start date parameter to EventFrom. Set End date parameter to EventTo. Click Apply. Add Calculated Fields Now, let\u0026rsquo;s add calculated fields for day of the week and hour of the day, then visualize them using a scatter plot.\nClick the Data menu and select Add Calculated Field. Name the field: event_day_of_week. Enter the formula: extract(\u0026lsquo;WD\u0026rsquo;, {event_date_time}) Click Save Add another calculated field with the following attributes: Calculated field name: event_hour_of_day Formula: extract(\u0026lsquo;HH\u0026rsquo;,{event_date_time}) Click Visualize icon from the Tool bar and choose Add visual. For field type, select the scatter plot.\nIn the Fields list, click the following attributes to set the graph attributes:\nX-axis: \u0026ldquo;event_hour_of_day\u0026rdquo; Y-axis: \u0026ldquo;event_day_of_week\u0026rdquo; Size: \u0026ldquo;ticket_price\u0026rdquo; You can now click on Publish on the top right corner of screen to publish your visuals. You can also, share it by clicking on the Share menu.\n"
},
{
	"uri": "/",
	"title": "ETL &amp; Data Lake Immersion",
	"tags": [],
	"description": "",
	"content": "ETL \u0026amp; Data Lake Immersion üîç What is a ETL \u0026amp; Data Lake Immersion? The ETL \u0026amp; Data Lake Immersion is an interactive, hands-on workshop designed to guide you through key processes in the world of data engineering. You\u0026rsquo;ll dive into critical stages like data ingestion, hydration, exploration, and consumption within a data lake on AWS.\nüéØ Benefits of a ETL \u0026amp; Data Lake Immersion This immersive experience provides you with real-world experience on how to use AWS analytics services to build a data lake. By participating, you\u0026rsquo;ll get to practice with the following tools and services:\nAmazon Kinesis Services\nCapture and analyze streaming data in real-time.\nAWS Data Migration Service\nEfficiently ingest batch data into your data lake.\nAWS Glue\nManage and catalog data as well as perform ETL processes.\nAmazon Athena\nQuery large datasets directly in your data lake with SQL-like syntax.\nAmazon QuickSight\nUse powerful data visualization to uncover insights from your data.\nBy the end of this workshop, you‚Äôll have gained practical skills to build a cloud-native, future-proof, and serverless data lake on AWS!\nüöÄ Ready to get started? Let‚Äôs dive in!\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]